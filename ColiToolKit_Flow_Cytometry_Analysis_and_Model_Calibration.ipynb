{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de2fe087-7f1c-4a65-a465-883490b866a9",
   "metadata": {},
   "source": [
    "# Coli Toolkit (CTK) Flow Cytometry Analysis and Model Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c6111-1ace-4e9a-9b74-528e38a93d08",
   "metadata": {},
   "source": [
    "## User Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd11242-df10-4cec-8f7a-92994d1fc167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import FlowCal as fcs\n",
    "import os\n",
    "from scipy.stats import gaussian_kde\n",
    "import shutil\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374249d0-9e44-4f00-b39a-f8b5a85fe7f7",
   "metadata": {},
   "source": [
    "### Data Source Setup\n",
    "Here you can link the directories containing the respective data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee31f43-ca7b-449c-8ace-8ef0225e5a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/\"\n",
    "for rep_dir in os.listdir(data_dir):\n",
    "    rep_dir_path = os.path.join(data_dir, rep_dir)\n",
    "    if not os.path.isdir(rep_dir_path):\n",
    "        continue    \n",
    "    if not rep_dir.startswith(\"replicate\"):\n",
    "        continue\n",
    "    \n",
    "    for const_dir in os.listdir(rep_dir_path):\n",
    "        const_dir_path = os.path.join(rep_dir_path, const_dir)\n",
    "        if not os.path.isdir(const_dir_path):\n",
    "            continue   \n",
    "\n",
    "        target_dir = os.path.join(data_dir, const_dir, rep_dir)\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        for file in os.listdir(const_dir_path):\n",
    "            file_path = os.path.join(const_dir_path, file)\n",
    "            if not os.path.isfile(file_path):\n",
    "                continue\n",
    "\n",
    "            target_file_path = os.path.join(target_dir, file)\n",
    "            shutil.copyfile(file_path, target_file_path)\n",
    "            print(f\"Copied {file_path} to {target_file_path}\")\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83aef74-097c-4329-92d3-9fc745c4d255",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_constitutive = \"data/constitutive/\"\n",
    "data_dir_inputs = \"data/inputs/\"\n",
    "data_dir_gates = \"data/gates/\"\n",
    "data_dir_reference = \"data/reference/\"\n",
    "data_dir_basal = \"data/basal/\"\n",
    "data_dir_inputs_cross_reactivity = \"data/inputs_cross_reactivity/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf4c403-fee7-47e8-8554-3656f61bb950",
   "metadata": {},
   "source": [
    "## Figure Output Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b80541c-1bed-4032-a15d-5fabce5b3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_dir = \"figures/\"\n",
    "figure_extension = \".pdf\"\n",
    "\n",
    "os.makedirs(figure_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ee1f4-bd2c-4ab4-aa8e-7ecafb3cc3e7",
   "metadata": {},
   "source": [
    "### General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75513b6e-72a9-40ec-b7a8-40362400045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_promoter = \"J23101\"                                           # The constitutive reference promoter which serves as baseline for the RPU conversion\n",
    "alternative_reference_promoter_name = {\"J23101\": \"pJCM434\"}             # Currently, the reference promoter is part of the dataset in two different names\n",
    "inducer_units = {\"ind1\": \"pgul\", \"ind2\": \"uM\", \"ind3\": \"uM\"}\n",
    "inducer_molecule_type = {\"pJCM435\": \"IPTG\", \"pJCM448\": \"Ara\", \"pJCM449\": \"aTc\"}\n",
    "controlling_input_sensor = \"pJCM435\"                                    # The input sensor used for the characterization of the gate plasmids\n",
    "facs_channel = \"FL1-A\"                                                  # The fluorescence FACS channel we are interested in\n",
    "p_gating = 0.95                                                         # The probability mass to preserve after gating\n",
    "gating_channels = [\"FSC-A\", \"FSC-H\"]                                    # The two channels to apply gating on\n",
    "kde_bandwith = 0.05                                                     # Bandwith of gaussian kernel density estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905cec82-31e2-462a-8725-ea4676d092a4",
   "metadata": {},
   "source": [
    "## Model Calibration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879e033a-a0b3-4c1c-9aca-8eff58d898b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_walkers = 10\n",
    "n_chains = 10\n",
    "n_samples = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b3084-8b77-4d95-af15-5d8f92b757c6",
   "metadata": {},
   "source": [
    "### Figures Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f20e8e-4b35-4561-b858-131faab53686",
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = (10**(-3), 10**2)\n",
    "histogram_bins = np.logspace(np.log10(lim[0]), np.log10(lim[1]), 200)             \n",
    "hist_as_density = False                                                # Whether to turn the histogram into a valid density (sum over width x height = 1) or not\n",
    "plot_as_density = False                                                 # Whether to use kernel density estimate instead of histograms for representing distributions\n",
    "\n",
    "input_sensor_order = [\"Ptac\", \"PBAD\", \"Ptet\"]\n",
    "inducer_order = [\"No\", \"IPTG\", \"Ara\", \"aTc\"]\n",
    "present_top_k = 5                                                      # Number of top gates to present according to dynamic range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d102b19f-d11c-4e1d-aa6d-40b7ddf921ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "font = {'family': 'serif',\n",
    "        'serif': ['Helvetica'],\n",
    "        'size': 8}\n",
    "\n",
    "savefig = {'bbox': 'tight',\n",
    "           'pad_inches': 0.01,\n",
    "           'dpi': 1200,\n",
    "           'transparent': True}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('savefig', **savefig)\n",
    "mm_to_inch = lambda val: np.array(val) * 0.0393701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a94849-face-4dfe-9146-2b8bd8269fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [(0.30, 0.56, 1.00), (0.35, 0.24, 1.00), (1.00, 0.00, 0.42), (1.00, 0.40, 0.10), (1.00, 0.65, 0.19)]\n",
    "COLORS_DARK = [(0.00, 0.24, 0.90), (0.28, 0.00, 0.84), (0.80, 0.00, 0.27), (0.90, 0.24, 0.00), (1.00, 0.50, 0.00)]\n",
    "COLORS_MAIN = [(0.30, 0.55, 1.00), (1.00, 0.40, 0.10)]\n",
    "COLORS_CMAP_ORANGE = [\"#FFFFFF\", \"#FF5500\", \"#B3003C\"]\n",
    "COLORS_CMAP_BLUE = [\"#FFFFFF\", \"#69A3FF\", \"#4400D6\"]\n",
    "\n",
    "COLORMAP_ORANGE = LinearSegmentedColormap.from_list(\"my_cmap\", COLORS_CMAP_ORANGE)\n",
    "COLORMAP_BLUE = LinearSegmentedColormap.from_list(\"my_cmap\", COLORS_CMAP_BLUE)\n",
    "\n",
    "COLOR_GRAY = \"#808080\"\n",
    "COLOR_REPLICATES = COLORS_DARK[2:] # Use only redish colors for replicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffdd693-3bd7-42db-8ea0-94a284f9b66d",
   "metadata": {},
   "source": [
    "## Definition of Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8c992-ab3d-446e-9714-d91a5ac578c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_facs_files(directory):\n",
    "    replicate_directories = os.listdir(directory)                    # Gets all the elements in data_dir and stores them as list in files \n",
    "\n",
    "    relevant_files = []                             # Target list to store the relevant files in\n",
    "    for rep_dir in replicate_directories:\n",
    "        rep_dir_path = os.path.join(directory, rep_dir)\n",
    "        if not os.path.isdir(rep_dir_path):\n",
    "            continue\n",
    "            \n",
    "        files = os.listdir(rep_dir_path)\n",
    "        # We here filter for .fcs files and store them in relevant_files        \n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[-1] != \".fcs\":    # Exclude not .fcs files            \n",
    "                continue\n",
    "            relevant_files.append(os.path.join(rep_dir_path, file))                 # Adds file to the list of relevant files\n",
    "            # break\n",
    "    return relevant_files\n",
    "\n",
    "def merge_replicates(replicates_data):\n",
    "    # Compare replicates via median values and discard whole dataset in case the replicates deviation is too large.    \n",
    "    merged_data = None\n",
    "    num_replicates = len(replicates_data)\n",
    "\n",
    "    if num_replicates < 1:\n",
    "        return merged_data\n",
    "        \n",
    "    num_levels = len(replicates_data[0])\n",
    "\n",
    "    merged_data = [None] * num_levels\n",
    "    merged_info = [None] * num_levels\n",
    "    for iL in range(num_levels):\n",
    "        dist_mat = np.zeros(shape=(num_replicates, num_replicates))\n",
    "        print(\"Merge\", iL, list(map(np.median, [replicate[iL] for replicate in replicates_data])))\n",
    "        replicate_lengths = np.array([len(replicate[iL]) for replicate in replicates_data])\n",
    "        replicates_to_consider = replicate_lengths >= 1000     # Require at least 1000 samples\n",
    "        \n",
    "        for iR1 in range(num_replicates):            \n",
    "            rep_val_1 = np.median(replicates_data[iR1][iL])            \n",
    "            for iR2 in range(num_replicates):\n",
    "                if iR2 < iR1:\n",
    "                    continue\n",
    "                    \n",
    "                rep_val_2 = np.median(replicates_data[iR2][iL])\n",
    "                dist_mat[iR1, iR2] = rep_val_1 / rep_val_2\n",
    "                dist_mat[iR2, iR1] = dist_mat[iR1, iR2]\n",
    "        \n",
    "        relative_distances = np.abs(np.log2(dist_mat))\n",
    "        vals = np.triu(relative_distances, k=1)\n",
    "        threshold = 3\n",
    "        mask = relative_distances <= threshold\n",
    "        use_replicates = np.all(mask[replicates_to_consider][:, replicates_to_consider])      # Only consider replicates with sufficient read counts\n",
    "        indices_to_consider = np.logical_and(replicates_to_consider, use_replicates)\n",
    "        \n",
    "            \n",
    "        print(\"Merge\", iL, indices_to_consider)\n",
    "        cur_data = [replicates_data[iR][iL] for iR, flag in enumerate(indices_to_consider) if flag]\n",
    "        if len(cur_data) > 0:\n",
    "            merged_data[iL] = np.concatenate(cur_data)\n",
    "        else:\n",
    "            merged_data[iL] = np.array([])\n",
    "        merged_info[iL] = indices_to_consider\n",
    "    return merged_data, merged_info\n",
    "                \n",
    "    \n",
    "\n",
    "def transform_data_dict(data_dict, construct_inducer_units={}):\n",
    "    for construct_id in data_dict:\n",
    "        print(construct_id)\n",
    "        replicates = {}\n",
    "        for replicate_id in data_dict[construct_id]:\n",
    "            cur_data = data_dict[construct_id][replicate_id]\n",
    "            replicate_inducer_unit = \"\"\n",
    "            replicate_inducer_molecule = \"\"\n",
    "            if construct_id in construct_inducer_units:\n",
    "                replicate_inducer_unit = construct_inducer_units[construct_id][\"unit\"]\n",
    "                replicate_inducer_molecule = construct_inducer_units[construct_id][\"molecule\"]\n",
    "                \n",
    "            replicate_inducer_concentrations = list(cur_data.keys())\n",
    "            replicate_inducer_concentrations = sorted(replicate_inducer_concentrations)            \n",
    "            \n",
    "            fcs_datasets = [cur_data[conc] for conc in replicate_inducer_concentrations]\n",
    "            replicate_data = [fcs_data[:, facs_channel] for fcs_data in fcs_datasets]\n",
    "            replicates[replicate_id] = {\"inducer unit\": replicate_inducer_unit,\n",
    "                                        \"inducer molecule\": replicate_inducer_molecule,\n",
    "                                        \"inducer concentrations\": replicate_inducer_concentrations, \n",
    "                                        \"data\": replicate_data}\n",
    "\n",
    "        replicate_inducer_units = [replicates[rep_id][\"inducer unit\"] for rep_id in replicates]\n",
    "        replicate_inducer_molecules = [replicates[rep_id][\"inducer molecule\"] for rep_id in replicates]\n",
    "        if len(set(replicate_inducer_units)) > 1:\n",
    "            raise Exception(\"Replicates have different inducer units\")\n",
    "        if len(set(replicate_inducer_molecules)) > 1:\n",
    "            raise Exception(\"Replicates have different inducer molecules\")\n",
    "            \n",
    "        inducer_unit = replicate_inducer_units[0]\n",
    "        inducer_molecule = replicate_inducer_molecules[0]\n",
    "\n",
    "        replicate_inducer_concentrations = [replicates[rep_id][\"inducer concentrations\"] for rep_id in replicates]\n",
    "        if not all([elem == replicate_inducer_concentrations[0] for elem in replicate_inducer_concentrations]):\n",
    "            raise Exception(\"Not all replicates have the same inducer concentrations\")\n",
    "        \n",
    "        inducer_concentrations = replicate_inducer_concentrations[0]\n",
    "\n",
    "        replicates_data = [replicates[rep_id][\"data\"] for rep_id in replicates]\n",
    "        merged_data, merged_info = merge_replicates(replicates_data)        \n",
    "        data = merged_data\n",
    "        \n",
    "        data_dict[construct_id] = {\"inducer unit\": inducer_unit,\n",
    "                                   \"inducer molecule\": inducer_molecule,\n",
    "                                   \"inducer concentrations\": inducer_concentrations,\n",
    "                                   \"data unit\": \"AU\",\n",
    "                                   \"data\": data,\n",
    "                                   \"replicates merge info\": merged_info, \n",
    "                                   \"replicates\": replicates}\n",
    "\n",
    "def load_constitutive_data(file_paths):\n",
    "    data_dict = {}             # Dictionary for storing the data with the inducer level as key\n",
    "    for path in file_paths:                                              # Iterate over the facs files\n",
    "        facs_data = fcs.io.FCSData(path)                                 # Use library to load facs data    \n",
    "        facs_data = fcs.gate.density2d(facs_data,                        # Gate facs data\n",
    "                                           channels=gating_channels,\n",
    "                                           gate_fraction=p_gating)       \n",
    "        # Extract and parse file name\n",
    "        replicate_id = os.path.basename(os.path.split(path)[0])\n",
    "        filename = os.path.basename(path)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        if not name in data_dict:\n",
    "            data_dict[name] = {}\n",
    "        data_dict[name][replicate_id] = {np.nan: facs_data}\n",
    "        print(f\"Loaded data for `{name}` ({replicate_id})\")\n",
    "\n",
    "\n",
    "    transform_data_dict(data_dict)\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def load_inducible_data(file_paths):\n",
    "    \n",
    "    data_dict = {}             # Dictionary for storing the data with the inducer level as key\n",
    "    construct_inducer_units = {}\n",
    "    \n",
    "    for path in file_paths:                                              # Iterate over the facs files\n",
    "        facs_data = fcs.io.FCSData(path)                                 # Use library to load facs data    \n",
    "        facs_data = fcs.gate.density2d(facs_data,                        # Gate facs data\n",
    "                                           channels=gating_channels,\n",
    "                                           gate_fraction=p_gating)\n",
    "        \n",
    "        # Extract and parse file name\n",
    "        replicate_id = os.path.basename(os.path.split(path)[0])\n",
    "        filename = os.path.basename(path)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "    \n",
    "        # Extract and parse file name\n",
    "        # Exemplary content of filename: pJCM435 0uM.fcs\n",
    "        # Format is: \"[construct_id] [inducer_level][inducer_unit].fcs\"\n",
    "        construct_id, inducer_description = name.split(\" \")\n",
    "        \n",
    "        inducer_unit = None        \n",
    "        for unit in inducer_units.values():      \n",
    "            \n",
    "            if unit in inducer_description:\n",
    "                inducer_unit = unit\n",
    "        if inducer_unit is None:\n",
    "            raise Exception(f\"Unknow inducer unit in {inducer_description}\")\n",
    "    \n",
    "        inducer_concentration = float(inducer_description.replace(inducer_unit, \"\"))\n",
    "        \n",
    "        if not construct_id in data_dict:\n",
    "            data_dict[construct_id] = {}\n",
    "            construct_inducer_units[construct_id] = {\"molecule\": inducer_molecule_type[construct_id] if construct_id in inducer_molecule_type else \"\", \n",
    "                                                     \"unit\": inducer_unit}\n",
    "            \n",
    "        if not replicate_id in data_dict[construct_id]:\n",
    "            data_dict[construct_id][replicate_id] = {}\n",
    "        \n",
    "        data_dict[construct_id][replicate_id][inducer_concentration] = facs_data\n",
    "        \n",
    "        print(f\"Loaded data for construct `{construct_id}` and inducer concentration {inducer_concentration} {inducer_unit} ({replicate_id})\")\n",
    "    \n",
    "    \n",
    "    transform_data_dict(data_dict, construct_inducer_units)\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94a44b-c939-420a-aac7-b0e09b19c192",
   "metadata": {},
   "source": [
    "## Load and Gate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad30e276-bad7-4941-af95-e3ed14c59794",
   "metadata": {},
   "source": [
    "### Load Constitutive Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63736b62-3739-410b-b542-aedf80342abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We here infer the inducer concentration from the file names and load the facs data\n",
    "\n",
    "file_paths = list_facs_files(data_dir_constitutive)\n",
    "file_paths = sorted(file_paths)\n",
    "# file_paths = [path for path in file_paths if \"J23107\" in path]   # Exemplary code for loading data of a single construt only\n",
    "\n",
    "data_constitutive = load_constitutive_data(file_paths)\n",
    "print(data_constitutive)\n",
    "rpu_reference_data = data_constitutive[reference_promoter + \"-No\"][\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5f02c6-3189-41f8-a5cd-7dfd6575ff72",
   "metadata": {},
   "source": [
    "### Load Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e3201-3a65-435d-84de-79b024dba327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We here infer the inducer concentration from the file names and load the facs data\n",
    "\n",
    "file_paths = list_facs_files(data_dir_inputs)\n",
    "file_paths = sorted(file_paths)\n",
    "\n",
    "data_inputs = load_inducible_data(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ace8a-3729-4360-9d51-9b7259ecf7c5",
   "metadata": {},
   "source": [
    "### Load Gate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff815a-ab1d-4fca-af6d-4776bafd400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = list_facs_files(data_dir_gates)\n",
    "file_paths = sorted(file_paths)\n",
    "# file_paths = [path for path in file_paths if \"pJCM212\" in path] # Exemplary code for loading data of a single gate construct\n",
    "\n",
    "data_gates = load_inducible_data(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f2f527-ed4b-4ad0-8179-8bdb9cee5054",
   "metadata": {},
   "source": [
    "### Load Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ea3ed-6dbe-4f4c-8e64-825ee6d4a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We here infer the inducer concentration from the file names and load the facs data\n",
    "\n",
    "file_paths = list_facs_files(data_dir_reference)\n",
    "file_paths = sorted(file_paths)\n",
    "\n",
    "data_reference = load_inducible_data(file_paths)\n",
    "ref_name = alternative_reference_promoter_name[reference_promoter]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12681e-e7d5-4bec-a6dc-01ce6649a4f1",
   "metadata": {},
   "source": [
    "### Load Autofluorescence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca9d0a-c064-4874-a2a3-a352337d57fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = list_facs_files(data_dir_basal)\n",
    "file_paths = sorted(file_paths)\n",
    "\n",
    "data_basal = load_inducible_data(file_paths)\n",
    "autofluorescence_reference_data = np.concatenate((data_basal[\"DH10B\"][\"data\"])) # Use data from all inducer concentrations as reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6a5187-161b-43eb-8d18-843997550dda",
   "metadata": {},
   "source": [
    "### Load Input Cross Reactivitiy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de4fab6-2206-4886-8e38-c32299ab303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We here infer the inducer concentration from the file names and load the facs data\n",
    "\n",
    "file_paths = list_facs_files(data_dir_inputs_cross_reactivity)\n",
    "file_paths = sorted(file_paths)\n",
    "\n",
    "data_inputs_cross_reactivity = load_constitutive_data(file_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "985579cd-baab-4a5d-a20f-08ff8130fbf2",
   "metadata": {},
   "source": [
    "## Normalize Data to RPU\n",
    "\n",
    "Equation for the scaling factor $c$ used to rescale the fluorescence values into RPU.\n",
    "\n",
    "\\begin{align*}\n",
    "c = \\frac{\\bar{FP} - \\bar{FP}_0}{\\bar{FP}} \\, \\frac{1}{\\bar{FP}_{RPU} - \\bar{FP}_0}\n",
    "\\end{align*}\n",
    "\n",
    "In particular, $\\bar{FP}$ is the median fluorescence of the expression values under consideration. $\\bar{FP}_0$ is the median autofluorescence and $\\bar{FP}_{RPU}$ is the median fluorescence of the reference plasmid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9046748c-64e5-4c51-8459-b49ca66d76ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_RPU(data_dict, rpu_reference_data, autofluorescence_reference_data):\n",
    "    def rpu_transform(data, FP_0, FP_RPU):\n",
    "        output_data = [None] * len(data)\n",
    "        for iX in range(len(data)):\n",
    "            FP = np.median(data[iX])\n",
    "            \n",
    "            c = (FP - FP_0)/FP * 1 / (FP_RPU - FP_0)\n",
    "            \n",
    "            output_data[iX]= data[iX] * c        \n",
    "        return output_data\n",
    "        \n",
    "    FP_RPU = np.median(rpu_reference_data)\n",
    "    FP_0 = np.median(autofluorescence_reference_data)\n",
    "    # print(FP_RPU)\n",
    "    for construct_id in data_dict:\n",
    "        cur_entry = data_dict[construct_id]\n",
    "  \n",
    "            \n",
    "        data = cur_entry[\"data\"]        \n",
    "        cur_entry[\"output\"] = rpu_transform(data, FP_0, FP_RPU)\n",
    "        if \"replicates\" in cur_entry:\n",
    "            for replicate_id in cur_entry[\"replicates\"]:\n",
    "                data =  cur_entry[\"replicates\"][replicate_id][\"data\"]        \n",
    "                cur_entry[\"replicates\"][replicate_id][\"output\"] = rpu_transform(data, FP_0, FP_RPU)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f87ceab-d997-4aa0-9ffd-c9b0b70f21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_to_RPU(data_constitutive, rpu_reference_data, autofluorescence_reference_data)\n",
    "transform_to_RPU(data_inputs, rpu_reference_data, autofluorescence_reference_data)\n",
    "transform_to_RPU(data_gates, rpu_reference_data, autofluorescence_reference_data)\n",
    "transform_to_RPU(data_reference, rpu_reference_data, autofluorescence_reference_data)\n",
    "transform_to_RPU(data_basal, rpu_reference_data, autofluorescence_reference_data)\n",
    "transform_to_RPU(data_inputs_cross_reactivity, rpu_reference_data, autofluorescence_reference_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d1808c-f5b9-4603-9105-c52053b3f3eb",
   "metadata": {},
   "source": [
    "## Insert Inducer RPU into Controlled Constructs\n",
    "In particular, the median RPU of the input sensor used for characterizing is added to the controlled constructs data as second input reference. The original inducer concentration will be preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422df3ee-842e-401c-a226-63098e2e04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_input_rpu(data_dict, input_data):\n",
    "    median_input_RPU = [np.median(elem) for elem in input_data[\"output\"]]\n",
    "    for construct_id in data_dict:\n",
    "        data_dict[construct_id][\"input RPU\"] = median_input_RPU  \n",
    "        # Theoretically, one could assign the RPU histograms instead of the median value.\n",
    "        # However, the combined data wouldn't represent actuall joint data as the data has been generated in two separate experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ba370-10b5-4009-a1d1-4316fdd73b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = data_inputs[controlling_input_sensor]\n",
    "\n",
    "insert_input_rpu(data_gates, input_data=input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a02ba9c-8945-47e0-b3ea-dabe90bc84a7",
   "metadata": {},
   "source": [
    "# Model Calibration\n",
    "For the model calibration, we use the inhibitory Hill equation as model for the response curve and Parallel Tempering, a Markov chain Monte Carlo algorithm, for parameter sampling and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca1d7b-f0e1-46b1-b9d3-aa83d91e39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class OptimizationAlgorithm:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        raise Exception(\"Needs to be implemented by sub class\")\n",
    "\n",
    "class ParallelTempering(OptimizationAlgorithm):\n",
    "\n",
    "    def __init__(self, log_likelihood, log_prior, n_dim, n_walkers=1, n_chains=10):\n",
    "        self.log_likelihood = log_likelihood\n",
    "        self.log_prior = log_prior\n",
    "        self.n_dim = n_dim\n",
    "\n",
    "        self.n_walkers = n_walkers\n",
    "        self.n_chains = n_chains\n",
    "\n",
    "        swap_mask = np.zeros(shape=(n_walkers, int(np.ceil(n_chains / 2) * 2)), dtype=bool)\n",
    "        swap_mask[:, ::2] = 1\n",
    "        self.swap_mask = swap_mask\n",
    "        pass\n",
    "\n",
    "    def run(self, initial_parameters, n_samples=10 ** 3, target_acceptance_ratio=None,\n",
    "            adaptive_temperature=True):\n",
    "\n",
    "        n_walkers = self.n_walkers\n",
    "        n_chains = self.n_chains\n",
    "\n",
    "        initial_parameters = np.array(initial_parameters)\n",
    "        self.temperatures = np.power(2, np.arange(self.n_chains), dtype=float)\n",
    "\n",
    "        if adaptive_temperature and n_chains <= 2:\n",
    "            print(\n",
    "                f\"Disabling adaptive temperature for n_chains={n_chains}. Minimal number of chains for adaptive temperature is 3, but more chains are recommended.\")\n",
    "            adaptive_temperature = False\n",
    "\n",
    "        if adaptive_temperature:\n",
    "            self.temperatures[-1] = np.inf\n",
    "            # Value choice follows Vousden et al. 2016\n",
    "            v_factor = 10 ** 2\n",
    "            v = int(np.ceil(v_factor / n_walkers))\n",
    "            t0 = 10 * v  \n",
    "            S = np.log(np.diff(self.temperatures, axis=-1))\n",
    "            S = S[:-1]\n",
    "            # Diffs of T_2 - T_1, ..., T_(N-1) - T_(N-2). The diff T_N - T_(N-1) is excluded by purpose following 1 < i < N for the S_i\n",
    "\n",
    "        variance = 0.1\n",
    "        self.variance = np.ones(shape=(self.n_walkers, self.n_chains, self.n_dim))\n",
    "        self.variance = self.variance * np.expand_dims(np.expand_dims(np.arange(1, self.n_chains + 1), axis=0),\n",
    "                                                       axis=-1)\n",
    "        self.variance *= variance\n",
    "\n",
    "        adaptive_proposal_distribution = target_acceptance_ratio is not None and target_acceptance_ratio > 0 and target_acceptance_ratio < 1.0\n",
    "\n",
    "        adaptive_temperature_stop_iteration = int(n_samples / 2)\n",
    "\n",
    "        parameters = np.zeros(shape=(n_samples, n_walkers, n_chains, *initial_parameters.shape))\n",
    "        priors = np.zeros(shape=(n_samples, n_walkers, n_chains))\n",
    "        likelihoods = np.zeros(shape=(n_samples, n_walkers, n_chains))\n",
    "        step_accepts = np.zeros(shape=(n_samples, n_walkers, n_chains))\n",
    "        swap_accepts = []\n",
    "\n",
    "        params = np.array(initial_parameters)\n",
    "        likelihood = self.log_likelihood(params)\n",
    "        prior = self.log_prior(params)\n",
    "        # max_iN = 0\n",
    "        for iN in tqdm(range(n_samples)):\n",
    "            self.beta = 1 / np.expand_dims(self.temperatures, axis=0)\n",
    "\n",
    "            params, prior, likelihood, step_accept = self.step(params, prior, likelihood, index=iN)\n",
    "            swap_round = iN % 10 == 9\n",
    "            if swap_round:\n",
    "                params, prior, likelihood, swap_accept = self.swap(params, prior, likelihood, index=iN)\n",
    "                swap_accepts.append(swap_accept)\n",
    "\n",
    "            parameters[iN] = params\n",
    "            priors[iN] = prior\n",
    "            likelihoods[iN] = likelihood\n",
    "            step_accepts[iN] = step_accept\n",
    "\n",
    "            ##################################\n",
    "            # Adaptive Proposal Distribution #\n",
    "            ##################################\n",
    "            if adaptive_proposal_distribution and iN >= 100 and iN % 10 == 0:\n",
    "                # Considers Windowed average of the last 100 steps\n",
    "                acc_rate_deviation = step_accepts[max(iN - 100 + 1, 0):iN + 1] - target_acceptance_ratio\n",
    "                scaling_params = np.exp((np.mean(acc_rate_deviation, axis=0)))\n",
    "                self.variance = self.variance * np.expand_dims(scaling_params, axis=-1)\n",
    "\n",
    "            ###############################\n",
    "            # Adaptive Temperature Ladder #\n",
    "            ###############################\n",
    "\n",
    "            if adaptive_temperature and swap_round and iN > 20 and iN < adaptive_temperature_stop_iteration:\n",
    "                kappa = 1 / v * t0 / (iN + t0)\n",
    "                # Be aware that only every 10th iteration is a swap iteration\n",
    "                rel_accepts = swap_accepts[max(len(swap_accepts) - 100, 0):]  # Select relevant data\n",
    "                swap_acceptance_rate = np.mean(rel_accepts, axis=0)  # Average over multiple samples\n",
    "                swap_acceptance_rate = np.mean(swap_acceptance_rate, axis=0)  # Average over multiple walkers\n",
    "                swap_rate_diff = -np.diff(swap_acceptance_rate, axis=0)  # Compute the diff over the chains\n",
    "                S = S + kappa * swap_rate_diff\n",
    "                temp_diffs = self.temperatures\n",
    "                temp_diffs[1:-1] = np.exp(S)\n",
    "                self.temperatures = np.cumsum(temp_diffs)\n",
    "\n",
    "        parameters = np.array(parameters)\n",
    "        priors = np.array(priors)\n",
    "        likelihoods = np.array(likelihoods)\n",
    "        step_accepts = np.array(step_accepts)\n",
    "        swap_accepts = np.array(swap_accepts)\n",
    "        return parameters, priors, likelihoods, step_accepts, swap_accepts\n",
    "\n",
    "    def step(self, params, prior, likelihood, index):\n",
    "        move = np.random.normal(loc=0, scale=self.variance)\n",
    "        proposal = params + move\n",
    "\n",
    "        proposal_likelihood = self.log_likelihood(proposal)\n",
    "        proposal_prior = self.log_prior(proposal)\n",
    "        proposal_prob = self.beta * proposal_likelihood + proposal_prior\n",
    "\n",
    "        prob = self.beta * likelihood + prior\n",
    "\n",
    "        log_diff = proposal_prob - prob\n",
    "        diff = np.exp(log_diff)\n",
    "        u = np.random.uniform(size=(self.n_walkers, self.n_chains))\n",
    "        accept = u < diff\n",
    "\n",
    "        new_prior = np.where(accept, proposal_prior, prior)\n",
    "        new_likelihood = np.where(accept, proposal_likelihood, likelihood)\n",
    "\n",
    "        params_accepts = np.expand_dims(accept, -1)\n",
    "        new_params = np.where(params_accepts, proposal, params)\n",
    "        return new_params, new_prior, new_likelihood, accept\n",
    "\n",
    "    def swap(self, params, prior, likelihood, index):\n",
    "        log_diff = np.diff(likelihood, axis=-1)\n",
    "        beta_diff = -np.diff(self.beta, axis=-1)\n",
    "\n",
    "        log_criterion = beta_diff * log_diff\n",
    "        criterion = np.exp(log_criterion)\n",
    "        u = np.random.uniform(size=(self.n_walkers, self.n_chains - 1))\n",
    "        # Ensure in the accepts step that a single chain does not swap to both adjacent chains (it should be possible to check this by using np.diff(accept) which should not yield 0 at a position including a 1 in accept\n",
    "        proposed_accept = u < criterion\n",
    "        self.swap_mask = np.roll(self.swap_mask, 1)\n",
    "        swap_mask = self.swap_mask[:, :self.n_chains - 1]\n",
    "        accept = np.logical_and(proposed_accept, swap_mask)\n",
    "        # accept[:, i] defines whether to swap between chain i and i+1.\n",
    "\n",
    "        # swap_matrice_1 is accept matrice with an additional all zeros entry\n",
    "        swap_matrice_1 = np.concatenate((accept, np.zeros((self.n_walkers, 1))), axis=1)\n",
    "        swap_matrice_2 = np.roll(swap_matrice_1, 1, axis=1)\n",
    "\n",
    "        left_rolled_prior = np.roll(prior, -1, axis=1)\n",
    "        right_rolled_prior = np.roll(prior, 1, axis=1)\n",
    "        left_rolled_likelihood = np.roll(likelihood, -1, axis=1)\n",
    "        right_rolled_likelihood = np.roll(likelihood, 1, axis=1)\n",
    "        left_rolled_params = np.roll(params, -1, axis=1)\n",
    "        right_rolled_params = np.roll(params, 1, axis=1)\n",
    "\n",
    "        new_prior = np.where(swap_matrice_1, left_rolled_prior, prior)\n",
    "        new_prior = np.where(swap_matrice_2, right_rolled_prior, new_prior)\n",
    "        new_likelihood = np.where(swap_matrice_1, left_rolled_likelihood, likelihood)\n",
    "        new_likelihood = np.where(swap_matrice_2, right_rolled_likelihood, new_likelihood)\n",
    "        new_params = np.where(np.expand_dims(swap_matrice_1, -1), left_rolled_params, params)\n",
    "        new_params = np.where(np.expand_dims(swap_matrice_2, -1), right_rolled_params, new_params)\n",
    "\n",
    "        return new_params, new_prior, new_likelihood, accept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205ab402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSampling(OptimizationAlgorithm):\n",
    "\n",
    "    def __init__(self, loss_func):\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "\n",
    "    def run(self, repetitions, initial_parameters, parameter_lower_bounds, parameter_upper_bounds):\n",
    "        \n",
    "        initial_parameters = np.array(initial_parameters)\n",
    "        parameter_lower_bounds = np.array(parameter_lower_bounds)\n",
    "        parameter_upper_bounds = np.array(parameter_upper_bounds)\n",
    "\n",
    "        assert initial_parameters.shape == parameter_upper_bounds.shape and parameter_upper_bounds.shape == parameter_lower_bounds.shape\n",
    "\n",
    "        shape = initial_parameters.shape\n",
    "        best_solution = None\n",
    "        best_score = np.inf\n",
    "\n",
    "        bounds_diff = parameter_upper_bounds - parameter_lower_bounds\n",
    "\n",
    "        for _ in tqdm(range(repetitions)):\n",
    "\n",
    "            new_proposal = np.random.uniform(low=0, high=1, size=shape)\n",
    "            new_proposal = parameter_lower_bounds + (new_proposal * (bounds_diff))\n",
    "\n",
    "            score = self.loss_func(new_proposal)\n",
    "\n",
    "            if score <= best_score:\n",
    "                best_solution = new_proposal\n",
    "                best_score = score\n",
    "        \n",
    "        return best_solution, best_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd41172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "class ScipyOptimization(OptimizationAlgorithm):\n",
    "\n",
    "    def __init__(self, loss_func):\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "\n",
    "    def run(self, initial_parameters, method=\"powell\", bounds=None, tol=10 ** (-10), options = {}):\n",
    "\n",
    "        result = minimize(self.loss_func, x0 = initial_parameters, method=method, tol = tol, bounds=bounds, options=options)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4eea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvolutionaryOptimization(OptimizationAlgorithm):\n",
    "\n",
    "    def __init__(self, loss_func):\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "        self.known_implementations = [\"random\"]\n",
    "\n",
    "    def run(self, parameter_lower_bounds, parameter_upper_bounds, groups = 10, group_size = 100, generations = 100, steps_per_generation=50, replacement_amount = 50, variance = 1, replacement_mode = \"random\"):\n",
    "        \n",
    "        shape = (groups, group_size, len(parameter_lower_bounds))\n",
    "        #init generations\n",
    "        generation_lower_bounds = np.tile(np.array(parameter_lower_bounds).reshape((1,1,len(parameter_lower_bounds))), (groups, group_size, 1))\n",
    "        generation_upper_bounds = np.tile(np.array(parameter_upper_bounds).reshape((1,1,len(parameter_upper_bounds))), (groups, group_size, 1))\n",
    "\n",
    "        generation_bounds_diff = generation_upper_bounds - generation_lower_bounds\n",
    "        generation_params = generation_lower_bounds + (np.random.uniform(low=0, high=1,size=(groups, group_size, len(parameter_lower_bounds))) * generation_bounds_diff)\n",
    "\n",
    "        #init generation scores\n",
    "        generation_score = np.apply_along_axis(self.loss_func, axis=2, arr=generation_params)\n",
    "\n",
    "        for _ in tqdm(range(generations)):\n",
    "            \n",
    "            #params anpassen\n",
    "            for _ in range(steps_per_generation):\n",
    "            \n",
    "                #Erstelle werte von 0.9-1.1 die dann als skalierungswert für die parameter verwendet werden\n",
    "                #multiplcative_values = np.clip(np.random.normal(loc=1.0, scale=variance, size=shape), a_min=0.9, a_max=1.1)\n",
    "                #generation_proposal = generation_params * multiplcative_values\n",
    "\n",
    "                move = np.random.normal(loc=0, scale=variance, size=shape)\n",
    "                generation_proposal = generation_params + move\n",
    "                \n",
    "                #berechne improvment aus neuen param vorschlag. differenz zwischen neuen und alten score. Wenn diff > 0 dann neuer score niedriger\n",
    "                proposal_score = np.apply_along_axis(self.loss_func, axis=2, arr=generation_proposal)\n",
    "                improvments = generation_score-proposal_score\n",
    "                mask = (improvments > 0)\n",
    "\n",
    "                #Ersetze alle elemente, wo der neue vorschlag besser war\n",
    "                generation_params[mask] = generation_proposal[mask]\n",
    "                generation_score[mask] = proposal_score[mask]\n",
    "            \n",
    "            #finde die parameter mit dem höchsten score\n",
    "            generation_replace_indices = np.argpartition(a=generation_score, axis=1, kth=replacement_amount)\n",
    "\n",
    "            \n",
    "\n",
    "            #gehe alle gruppen durch\n",
    "            for group in range(groups):\n",
    "                group_replace_indices = generation_replace_indices[group][replacement_amount+1:]\n",
    "                \n",
    "                possible_fill_in_indices = None\n",
    "\n",
    "                #case das mode \"random\" die indexe zum ersetzen auswählt\n",
    "                if replacement_mode == \"random\":\n",
    "                    possible_fill_in_indices = np.setdiff1d(ar1=np.arange(start=0, stop=group_size, step=1), ar2=group_replace_indices)\n",
    "                \n",
    "                else:\n",
    "                    raise NotImplementedError(f\"This replacement method is not implemented. Implemented are {self.known_implementations}!\")\n",
    "                \n",
    "                for replace_index in group_replace_indices:\n",
    "                    fill_in_index = np.random.choice(a=possible_fill_in_indices)\n",
    "                    generation_params[group][replace_index] = generation_params[group][fill_in_index]\n",
    "                    generation_score[group][replace_index] = generation_score[group][fill_in_index]\n",
    "        \n",
    "        minimal_index = np.argmin(a=generation_score)\n",
    "        shaped_minimal_index = np.unravel_index(minimal_index, generation_score.shape)\n",
    "\n",
    "        return generation_params[shaped_minimal_index[0]][shaped_minimal_index[1]], generation_score[shaped_minimal_index[0]][shaped_minimal_index[1]]\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ade96b-65e8-4ae6-97b7-251605073833",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a7bcb-8f44-4f3f-ab6d-0315a6436425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activatory_hill_equation(x, params):\n",
    "    ymax, ymin, n, k = params\n",
    "    y = ymin + (ymax - ymin) / (1 + np.power(k/x, n))\n",
    "    return y\n",
    "\n",
    "def inhibitory_hill_equation(x, params):\n",
    "    ymax, ymin, n, k = params\n",
    "    y = ymin + (ymax - ymin) / (1 + np.power(x/k, n))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a8c574-3986-4677-b382-de3083616f5a",
   "metadata": {},
   "source": [
    "## Define Loss Function Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf2ed4-38a1-4efd-96fa-48084d44b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_function(model, data):\n",
    "    def loss_function(params):\n",
    "        X, Y_true = data\n",
    "        losses = []\n",
    "        model_params = np.exp(params)        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            Y_pred = model(X, model_params)        \n",
    "        diffs = np.log(Y_pred) - np.log(Y_true)         \n",
    "        \n",
    "        losses = np.power(diffs, 2)  \n",
    "        nan_mask = np.logical_not(np.isnan(losses))\n",
    "        loss = np.sum(losses[nan_mask])        \n",
    "        \n",
    "        return loss\n",
    "        \n",
    "\n",
    "    return loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(all='ignore')\n",
    "\n",
    "for construct_id in data_inputs:    \n",
    "        \n",
    "    cur_data = data_inputs[construct_id]\n",
    "    inducer_concentration = cur_data[\"inducer concentrations\"]\n",
    "    output_RPU = cur_data[\"output\"]\n",
    "\n",
    "\n",
    "    data = inducer_concentration, list(map(lambda elem: np.median(elem), output_RPU))\n",
    "    X, Y_true = data\n",
    "    min_Y_true = np.nanmin(Y_true)\n",
    "    max_Y_true = np.nanmax(Y_true)\n",
    "    \n",
    "    #Hillkurve initialparameter\n",
    "    init_params = (min_Y_true, max_Y_true, 2, 1)\n",
    "    params_lower_bounds = (min_Y_true * 10**(-1), max_Y_true * 10**(-1), 10**(-1), 10**(-1))\n",
    "    params_upper_bounds = (min_Y_true * 10**(1), max_Y_true * 10**(1), 10**(1), 10**(1))\n",
    "    \n",
    "\n",
    "    loss_func = get_loss_function(model=activatory_hill_equation, data=data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    so = RandomSampling(loss_func=loss_func)\n",
    "    parameters, score = so.run(repetitions=2*10**6, initial_parameters=init_params, parameter_lower_bounds=params_lower_bounds, parameter_upper_bounds=params_upper_bounds)\n",
    "\n",
    "    print(f\"{construct_id=};{parameters=};{score=}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75381877",
   "metadata": {},
   "outputs": [],
   "source": [
    "for construct_id in data_inputs:    \n",
    "        \n",
    "    cur_data = data_inputs[construct_id]\n",
    "    inducer_concentration = cur_data[\"inducer concentrations\"]\n",
    "    output_RPU = cur_data[\"output\"]\n",
    "\n",
    "\n",
    "    data = inducer_concentration, list(map(lambda elem: np.median(elem), output_RPU))\n",
    "    X, Y_true = data\n",
    "    min_Y_true = np.nanmin(Y_true)\n",
    "    max_Y_true = np.nanmax(Y_true)\n",
    "    \n",
    "    #Hillkurve initialparameter\n",
    "    init_params = (min_Y_true, max_Y_true, 2, 1)\n",
    "\n",
    "    loss_func = get_loss_function(model=activatory_hill_equation, data=data)\n",
    "\n",
    "    max_fev_per_param = 500\n",
    "\n",
    "    tol=10 ** (-10)\n",
    "    options={\"disp\": True,\n",
    "            \"ftol\": 10 ** (-7),\n",
    "            \"maxfev\": len(init_params) * max_fev_per_param,\n",
    "            \"disp\":False\n",
    "            }\n",
    "    \n",
    "    so = ScipyOptimization(loss_func=loss_func)\n",
    "    result= so.run(initial_parameters=init_params, tol=tol, options=options)\n",
    "    print(f\"{construct_id=};parameters={result['x']}; score={result['fun']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a07623",
   "metadata": {},
   "outputs": [],
   "source": [
    "for construct_id in data_inputs:    \n",
    "        \n",
    "    cur_data = data_inputs[construct_id]\n",
    "    inducer_concentration = cur_data[\"inducer concentrations\"]\n",
    "    output_RPU = cur_data[\"output\"]\n",
    "\n",
    "\n",
    "    data = inducer_concentration, list(map(lambda elem: np.median(elem), output_RPU))\n",
    "    X, Y_true = data\n",
    "    min_Y_true = np.nanmin(Y_true)\n",
    "    max_Y_true = np.nanmax(Y_true)\n",
    "    \n",
    "    #Hillkurve initialparameter\n",
    "    init_params = (min_Y_true, max_Y_true, 2, 1)\n",
    "    params_lower_bounds = (min_Y_true * 10**(-1), max_Y_true * 10**(-1), 10**(-1), 10**(-1))\n",
    "    params_upper_bounds = (min_Y_true * 10**(1), max_Y_true * 10**(1), 10**(1), 10**(1))\n",
    "    \n",
    "\n",
    "    loss_func = get_loss_function(model=activatory_hill_equation, data=data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    eo = EvolutionaryOptimization(loss_func=loss_func)\n",
    "\n",
    "    groups = 1\n",
    "    group_size = 100\n",
    "    generations = 150\n",
    "    steps_per_generation = 300\n",
    "    replacement_amount = 20\n",
    "    variance = 0.001\n",
    "    params, score = eo.run(parameter_lower_bounds=params_lower_bounds, parameter_upper_bounds=params_upper_bounds,\n",
    "                    groups=groups, group_size=group_size, generations=generations, steps_per_generation=steps_per_generation, replacement_amount=replacement_amount, variance=variance, replacement_mode=\"random\")\n",
    "    \n",
    "    print(f\"{construct_id=}; {params=}; {score=}\")\n",
    "\n",
    "    #print(f\"{construct_id=}; {parameters=}; {score=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a1a8a5-4ad7-415e-ae6d-2b7565e83ac5",
   "metadata": {},
   "source": [
    "## Fit Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945dfb04-047d-491b-8292-6e59f713c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for construct_id in data_inputs:    \n",
    "        \n",
    "    cur_data = data_inputs[construct_id]\n",
    "    inducer_concentration = cur_data[\"inducer concentrations\"]\n",
    "    output_RPU = cur_data[\"output\"]\n",
    "\n",
    "\n",
    "    data = inducer_concentration, list(map(lambda elem: np.median(elem), output_RPU))\n",
    "    X, Y_true = data\n",
    "    min_Y_true = np.nanmin(Y_true)\n",
    "    max_Y_true = np.nanmax(Y_true)\n",
    "    \n",
    "    init_params = (min_Y_true, max_Y_true, 2, 1)\n",
    "    \n",
    "\n",
    "    loss_func = get_loss_function(model=activatory_hill_equation, data=data)\n",
    "    \n",
    "\n",
    "    def log_prior(params):\n",
    "        # Define more precise prior for y_max and y_min with a broad distribution (y_max, y_max * 2). Check how proxies for n and k can be inferred directly from data\n",
    "\n",
    "        a_min=np.log([max_Y_true, min_Y_true / 2, 10**(-20), 10**(-20)])\n",
    "        a_max=np.log([max_Y_true * 2, min_Y_true, 10**10, 10**10])\n",
    "        l_prior = - np.power(params - np.clip(params, a_min=a_min, a_max=a_max), 2) / 1\n",
    "        l_prior = np.sum(l_prior, axis=-1)    \n",
    "        \n",
    "        return l_prior        \n",
    "    \n",
    "    def log_likelihood(params):    \n",
    "        # Vectorization of loss function\n",
    "        if len(params.shape) == 1:\n",
    "            return -loss_func(params)\n",
    "        else:\n",
    "            log_likelihood_vals = np.empty(shape=params.shape[:2])\n",
    "            for i1 in range(params.shape[0]):\n",
    "               for i2 in range(params.shape[1]):\n",
    "                   log_likelihood_vals[i1, i2] = -loss_func(params[i1, i2]) / 1\n",
    "                \n",
    "            return log_likelihood_vals\n",
    "    \n",
    "    \n",
    "    \n",
    "    pt = ParallelTempering(log_likelihood=log_likelihood, log_prior=log_prior, n_dim=len(init_params), \n",
    "                           n_walkers=n_walkers,\n",
    "                           n_chains=n_chains)\n",
    "    parameters, priors, likelihoods, step_accepts, swap_accepts = pt.run(initial_parameters=np.log(init_params), n_samples=n_samples,\n",
    "                                                                             target_acceptance_ratio=0.4,\n",
    "                                                                             adaptive_temperature=True)\n",
    "    best_index = np.unravel_index(np.argmax(priors + likelihoods), likelihoods.shape)\n",
    "    params = np.exp(parameters[*best_index])\n",
    "\n",
    "    best_loss = loss_func(np.log(params))\n",
    "    # ymax, ymin, n, k\n",
    "    params_dict = {\"y_max\": params[0],\n",
    "                   \"y_min\": params[1],\n",
    "                   \"n\": params[2],\n",
    "                   \"k\": params[3]}\n",
    "    data_inputs[construct_id][\"model information\"] = {\"model type\": \"ACTIVATORY HILL EQUATION\",\n",
    "                                                     \"calibration loss\": best_loss,\n",
    "                                                     \"parameters\" : params_dict}\n",
    "    print(f\"{construct_id}; parameters={params_dict}; score={best_loss}\")\n",
    "    \n",
    "    \n",
    "    # params = init_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed970de-d23c-4cb0-88c1-e0c2b05b39f6",
   "metadata": {},
   "source": [
    "## Fit Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f21aee-ae73-4838-83db-40664d6d8991",
   "metadata": {},
   "outputs": [],
   "source": [
    "for construct_id in data_gates:\n",
    "    cur_data = data_gates[construct_id]\n",
    "    output_RPU = data_gates[construct_id]\n",
    "    input_RPU = cur_data[\"input RPU\"]\n",
    "    output_RPU = cur_data[\"output\"]\n",
    "\n",
    "    data = input_RPU, list(map(lambda elem: np.median(elem), output_RPU))\n",
    "    X, Y_true = data\n",
    "    min_Y_true = np.nanmin(Y_true)\n",
    "    max_Y_true = np.nanmax(Y_true)\n",
    "    \n",
    "    init_params = (max_Y_true, min_Y_true, 2, 0.01)\n",
    "    \n",
    "\n",
    "    loss_func = get_loss_function(model=inhibitory_hill_equation, data=data)\n",
    "    \n",
    "\n",
    "    def log_prior(params):\n",
    "        # Define more precise prior for y_max and y_min with a broad distribution (y_max, y_max * 2). Check how proxies for n and k can be inferred directly from data\n",
    "\n",
    "        a_min=np.log([max_Y_true, min_Y_true / 2, 10**(-20), 10**(-20)])\n",
    "        a_max=np.log([max_Y_true * 2, min_Y_true, 10**10, 10**10])\n",
    "        l_prior = - np.power(params - np.clip(params, a_min=a_min, a_max=a_max), 2) / 1\n",
    "        l_prior = np.sum(l_prior, axis=-1)    \n",
    "        \n",
    "        return l_prior        \n",
    "    \n",
    "    def log_likelihood(params):    \n",
    "        # Vectorization of loss function\n",
    "        if len(params.shape) == 1:\n",
    "            return -loss_func(params)\n",
    "        else:\n",
    "            log_likelihood_vals = np.empty(shape=params.shape[:2])\n",
    "            for i1 in range(params.shape[0]):\n",
    "               for i2 in range(params.shape[1]):\n",
    "                   log_likelihood_vals[i1, i2] = -loss_func(params[i1, i2]) / 1\n",
    "                \n",
    "            return log_likelihood_vals\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    pt = ParallelTempering(log_likelihood=log_likelihood, log_prior=log_prior, n_dim=len(init_params), \n",
    "                           n_walkers=n_walkers,\n",
    "                           n_chains=n_chains)\n",
    "    parameters, priors, likelihoods, step_accepts, swap_accepts = pt.run(initial_parameters=np.log(init_params), n_samples=n_samples,\n",
    "                                                                             target_acceptance_ratio=0.4,\n",
    "                                                                             adaptive_temperature=True)\n",
    "    best_index = np.unravel_index(np.argmax(priors + likelihoods), likelihoods.shape)\n",
    "    params = np.exp(parameters[*best_index])\n",
    "    print(f\"{params=}\")\n",
    "\n",
    "    best_loss = loss_func(np.log(params))\n",
    "    # ymax, ymin, n, k\n",
    "    params_dict = {\"y_max\": params[0],\n",
    "                   \"y_min\": params[1],\n",
    "                   \"n\": params[2],\n",
    "                   \"k\": params[3]}\n",
    "    data_gates[construct_id][\"model information\"] = {\"model type\": \"INHIBITORY HILL EQUATION\",\n",
    "                                                     \"calibration loss\": best_loss,\n",
    "                                                     \"parameters\" : params_dict}\n",
    "    print(f\"Fitted parameters for gate {construct_id} with loss {best_loss} ({params_dict})\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b7ac7f-cec5-4886-ad99-31df274964d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to visualize statistics of parallel tempering\n",
    "if False:\n",
    "    from numpy.lib.stride_tricks import sliding_window_view\n",
    "    step_accepts_sliding_window = np.transpose(sliding_window_view(step_accepts[:, :, :], 100, axis=0),\n",
    "                                               axes=(0, 3, 1, 2))\n",
    "    step_accepts_avg = np.mean(step_accepts_sliding_window, axis=1)\n",
    "    \n",
    "    swap_accepts_sliding_window = np.transpose(sliding_window_view(swap_accepts, 10, axis=0), axes=(0, 3, 1, 2))\n",
    "    swap_accepts_avg = np.mean(swap_accepts_sliding_window, axis=1)\n",
    "    \n",
    "    for iWalker in range(n_walkers):\n",
    "        fig, axes = plt.subplots(ncols=2)\n",
    "    \n",
    "        for iChain in range(step_accepts_avg.shape[-1]):\n",
    "            axes[0].plot(np.arange(2) * (len(step_accepts_avg) - 1),\n",
    "                         np.ones(2) * 0.4 + (n_chains - iChain - 1), \"k--\", alpha=0.5)\n",
    "            axes[0].plot(np.arange(len(step_accepts_avg)),\n",
    "                         step_accepts_avg[:, iWalker, iChain] + (n_chains - iChain - 1), label=iChain, alpha=1)\n",
    "    \n",
    "        for iChain in range(swap_accepts_avg.shape[-1]):\n",
    "            axes[1].plot(np.arange(2) * (len(swap_accepts_avg) - 1), np.ones(2) * 0 + (n_chains - iChain - 1), \"r--\",\n",
    "                         alpha=0.5)\n",
    "            axes[1].plot(np.arange(len(swap_accepts_avg)),\n",
    "                         swap_accepts_avg[:, iWalker, iChain] + (n_chains - iChain - 1), label=iChain, alpha=1)\n",
    "    \n",
    "        ylim = axes[0].get_ylim()\n",
    "        axes[1].set_ylim(ylim)\n",
    "        axes[0].legend()\n",
    "        axes[1].legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50d780-3dd6-480d-80ad-c0bf555b231e",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855f907-1b6b-4b4e-82d2-057df2d4e69b",
   "metadata": {},
   "source": [
    "## Visualize Constitutive Promoters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f20b7e-695a-479a-b442-5fea524761d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_median_pairs = [(key, np.median(data_constitutive[key][\"output\"])) for key in data_constitutive]\n",
    "key_median_pairs = sorted(key_median_pairs, key=lambda elem: elem[1], reverse=False)\n",
    "key_median_pairs = [elem[0] for elem in key_median_pairs]\n",
    "\n",
    "input_device_keys = [key.split(\"-\")[0] for key in key_median_pairs if \"No\" in key]\n",
    "inducer_keys = [key.split(\"-\")[1] for key in key_median_pairs]\n",
    "\n",
    "input_device_keys = input_device_keys\n",
    "inducer_keys = [elem for elem in inducer_order if elem in inducer_keys]\n",
    "\n",
    "data = []\n",
    "for iRow, input_device_key in enumerate(input_device_keys):\n",
    "    cur_data = []\n",
    "    for iCols, inducer_key in enumerate(inducer_keys):\n",
    "        cur_data.append(data_constitutive[f\"{input_device_key}-{inducer_key}\"])\n",
    "    data.append(cur_data)\n",
    "\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef4cc5-f663-4c87-a315-65d95eb628de",
   "metadata": {},
   "source": [
    "### Constitutive Promoters Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a60f17-00e0-47f0-86df-a8dc9a58df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_plot = list(map(lambda elem: elem[\"output\"], data[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae2b4e-2bda-4abe-896b-0767e03be00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (180, 90)\n",
    "fig, ax = plt.subplots(figsize=mm_to_inch(figsize))\n",
    "\n",
    "length = len(input_device_keys)\n",
    "color = COLORS_MAIN[0]\n",
    "\n",
    "for iR in range(length):\n",
    "    input_device = input_device_keys[length - iR - 1]\n",
    "    cur_data = data_to_plot[length - iR - 1][0]\n",
    "\n",
    "    if plot_as_density and True:            \n",
    "        kde = gaussian_kde(cur_data, bw_method=kde_bandwith)\n",
    "        kde_vals = kde( )\n",
    "        kde_vals = kde_vals / np.max(kde_vals) * 0.75\n",
    "        ax.plot(histogram_bins, kde_vals + iR, color=color)\n",
    "        ax.fill_between(histogram_bins, np.ones(shape=histogram_bins.shape) * iR, kde_vals + iR, color=color, alpha=0.5)\n",
    "    else:\n",
    "        hist, bins = np.histogram(cur_data, bins=histogram_bins)\n",
    "        hist = hist  / np.max(hist) * 1.5\n",
    "        ax.bar(bins[:-1], hist, np.diff(histogram_bins), color=color, alpha=0.5, align=\"edge\", bottom=iR)    \n",
    "        # ax.plot(bins, np.ones(len(bins)) * iR, color=color) \n",
    "        ax.plot(np.repeat(bins, 2)[1:-1], np.repeat(hist, 2) + iR, linewidth=0.6, color=color, solid_capstyle='butt', solid_joinstyle=\"miter\") \n",
    "        \n",
    "    \n",
    "    \n",
    "    ax.text(bins[0], iR + 0.1, f\"{input_device} ({np.median(cur_data):.2f})\", va='bottom', ha='left')#, fontsize=10)\n",
    "\n",
    "    ax.set_xlabel(\"[RPU]\")\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "\n",
    "# ax.plot(np.flip(list(map(np.median, data_to_plot))), range(length), color=COLORS_MAIN[0])\n",
    "    \n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "plt.savefig(f\"{figure_dir}/constitutive_promoters_histograms{figure_extension}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bb486d-8d83-43e5-8a50-da2cd0c66379",
   "metadata": {},
   "source": [
    "### Constitutive Promoters Cross Reactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2e25b-6ad4-493b-9819-22363771f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_plot = np.zeros(shape=data.shape)\n",
    "for iRow in range(data.shape[0]):\n",
    "    for iCol in range(data.shape[1]):\n",
    "        data_to_plot[iRow, iCol] = np.median(data[iRow, iCol][\"output\"])\n",
    "\n",
    "data_to_plot = data_to_plot.transpose()\n",
    "\n",
    "fig_size = np.array([180, 90])\n",
    "fig_size = mm_to_inch(fig_size)\n",
    "fig, ax = plt.subplots(figsize=fig_size)\n",
    "im = ax.imshow(data_to_plot, cmap=COLORMAP_BLUE, interpolation=\"none\", origin=\"upper\", rasterized=True, norm=matplotlib.colors.SymLogNorm(vmax=10**1, linthresh=0.001))\n",
    "\n",
    "\n",
    "x_ticks = np.arange(data_to_plot.shape[1])\n",
    "y_ticks = np.arange(data_to_plot.shape[0])\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_xticklabels(input_device_keys,rotation=45)\n",
    "ax.set_yticklabels(inducer_keys)\n",
    "ax.tick_params(top=True, labeltop=True, bottom=False, labelbottom=False)\n",
    "\n",
    "ax.set_xticks(np.arange(data_to_plot.shape[1] + 1) - 0.5, minor=True)\n",
    "ax.set_yticks(np.arange(data_to_plot.shape[0] + 1) - 0.5, minor=True)\n",
    "ax.grid(which=\"minor\", color=\"#808080\", linestyle='-', linewidth=1)\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=0.2, pad=0.1)\n",
    "cbar = plt.colorbar(im, cax=cax)\n",
    "cbar.set_label(\"Output [RPU]\")\n",
    "cbar.set_ticks([10**(-1), 10**0, 10**1])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{figure_dir}/constitutive_promoter_cross_reactivity{figure_extension}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b9d5bb-0260-4911-aca5-9ae2c1dcc7e5",
   "metadata": {},
   "source": [
    "### Save data to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ff685-0322-43ab-9259-af68b7a5c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rep = []\n",
    "\n",
    "header = [\"Device\"] + inducer_keys[:1]\n",
    "table_rep.append(header)\n",
    "\n",
    "length = len(input_device_keys)\n",
    "for iR in range(length):\n",
    "    input_device = input_device_keys[iR]\n",
    "    row_vals = [input_device]\n",
    "    # for iC in range(data.shape[1]):\n",
    "        # cur_data = data[iR][iC]\n",
    "    cur_data = data[iR][0]\n",
    "    cur_val = np.median(cur_data[\"output\"][0])\n",
    "    row_vals.append(f\"{cur_val:.4f}\")\n",
    "    table_rep.append(row_vals)\n",
    "    \n",
    "    \n",
    "table_str = \"\\n\".join([\", \".join(row) for row in table_rep])\n",
    "print(table_rep)\n",
    "with open(\"figures/constitutive_promoters_cross_reactivity.csv\", \"w\") as file:\n",
    "    file.write(table_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae1539-f487-40a3-91c8-bda9fa671f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize consistency of reference promoter\n",
    "ref_data = data_reference[\"pJCM434\"]\n",
    "const_data = data_constitutive[\"J23101-No\"]\n",
    "const_data_AU = const_data[\"data\"]\n",
    "\n",
    "figsize = (180, 60)\n",
    "\n",
    "inducer_concentration = ref_data[\"inducer concentrations\"]\n",
    "output_AU = ref_data[\"data\"]\n",
    "\n",
    "fig, axes = plt.subplots(ncols=len(inducer_concentration), sharey=True, figsize=mm_to_inch(figsize))\n",
    "fig.subplots_adjust(wspace=0)\n",
    "\n",
    "is_first = True\n",
    "iL = 0\n",
    "\n",
    "masks = np.array(cur_data[\"replicates merge info\"])\n",
    "for ax, ind_level, out_levels in zip(axes, inducer_concentration, output_AU):    \n",
    "\n",
    "\n",
    "    ax.hist(const_data_AU, \n",
    "        orientation=\"horizontal\", \n",
    "        color=COLORS_MAIN[1], \n",
    "        alpha=0.4,\n",
    "        bins=np.logspace(-1, 5, 200),#histogram_bins, \n",
    "        density=hist_as_density,\n",
    "        histtype=\"stepfilled\",\n",
    "        ec=\"k\")\n",
    "    \n",
    "    ax.hist(out_levels, \n",
    "        orientation=\"horizontal\", \n",
    "        color=COLORS_MAIN[0], \n",
    "        alpha=0.4,\n",
    "        bins=np.logspace(-1, 5, 200),#histogram_bins, \n",
    "        density=hist_as_density,\n",
    "        histtype=\"stepfilled\",\n",
    "        ec=\"k\")\n",
    "\n",
    "\n",
    "\n",
    "    # print(ind_level, np.mean(cur_data[ind_level]))\n",
    "\n",
    "    if False:\n",
    "        for iR, replicate_id in enumerate(cur_data[\"replicates\"]):\n",
    "            replicate_output_AU = cur_data[\"replicates\"][replicate_id][\"output\"]        \n",
    "            color = \"black\" if masks[iL, iR] else COLORS[2]\n",
    "            ax.hist(replicate_output_AU[iL], \n",
    "                    orientation=\"horizontal\", \n",
    "                    color=color, \n",
    "                    bins=histogram_bins, \n",
    "                    density=hist_as_density, \n",
    "                    alpha=0.3,\n",
    "                   histtype=\"stepfilled\",\n",
    "                   ec=\"k\")    \n",
    "\n",
    "    xlim = ax.get_xlim()\n",
    "    ax.set_xlim((xlim[0], xlim[1] * 1.2))\n",
    "    ax.set_yscale(\"log\")\n",
    "    # ax.set_title(inducer_concentration)\n",
    "    ax.set_xticks([])  \n",
    "\n",
    "    if is_first:\n",
    "       ax.set_ylabel(\"Fluorescence [AU]\")\n",
    "       ax.tick_params(axis='y', which='both', direction=\"in\", left=True)            \n",
    "       is_first = False\n",
    "    else:\n",
    "       ax.tick_params(axis='y', which='both', left=False)    \n",
    "        \n",
    "    iL += 1\n",
    "    ax.set_xlabel(f\"{ind_level}\")# {cur_data['inducer unit']}\")\n",
    "    \n",
    "plt.suptitle(construct_id)\n",
    "# plt.savefig(figure_dir + \"reference_\" + construct_id + \"_histograms\" + figure_extension)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d829e8f0-6384-419c-9a5d-2939a4a691b6",
   "metadata": {},
   "source": [
    "# Visualize Cross Reactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d5a11-8826-4a6d-a81c-eb35a3b50da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_reactivity_keys = list(data_inputs_cross_reactivity.keys())\n",
    "input_device_keys = set()\n",
    "inducer_keys = set()\n",
    "for key in cross_reactivity_keys:\n",
    "    input_device_key, inducer_key = key.split(\"-\")\n",
    "    input_device_keys.add(input_device_key)\n",
    "    inducer_keys.add(inducer_key)\n",
    "\n",
    "input_device_keys = [elem for elem in input_sensor_order if elem in input_device_keys]\n",
    "inducer_keys = [elem for elem in inducer_order if elem in inducer_keys]\n",
    "\n",
    "data = []\n",
    "for iRow, input_device_key in enumerate(input_device_keys):\n",
    "    cur_data = []\n",
    "    for iCols, inducer_key in enumerate(inducer_keys):\n",
    "        cur_data.append(data_inputs_cross_reactivity[f\"{input_device_key}-{inducer_key}\"])\n",
    "    data.append(cur_data)\n",
    "\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4622464-4a2a-4d96-b2a0-6d733ae1e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_plot = np.zeros(shape=data.shape)\n",
    "for iRow in range(data.shape[0]):\n",
    "    for iCol in range(data.shape[1]):\n",
    "        data_to_plot[iRow, iCol] = np.median(data[iRow, iCol][\"output\"])\n",
    "\n",
    "data_to_plot = data_to_plot.transpose()\n",
    "\n",
    "fig_size = (60, 42)\n",
    "fig_size = mm_to_inch(fig_size)\n",
    "fig, ax = plt.subplots(figsize=fig_size)\n",
    "im = ax.imshow(data_to_plot, cmap=COLORMAP_BLUE, interpolation=\"none\", origin=\"upper\", rasterized=True, norm=matplotlib.colors.SymLogNorm(linthresh=0.001))\n",
    "\n",
    "\n",
    "x_ticks = np.arange(data_to_plot.shape[1])\n",
    "y_ticks = np.arange(data_to_plot.shape[0])\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_xticklabels(input_device_keys)\n",
    "ax.set_yticklabels(inducer_keys)\n",
    "ax.tick_params(top=True, labeltop=True, bottom=False, labelbottom=False)\n",
    "\n",
    "ax.set_xticks(np.arange(data_to_plot.shape[1] + 1) - 0.5, minor=True)\n",
    "ax.set_yticks(np.arange(data_to_plot.shape[0] + 1) - 0.5, minor=True)\n",
    "ax.grid(which=\"minor\", color=\"#808080\", linestyle='-', linewidth=1)\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=0.1, pad=0.1)\n",
    "cbar = plt.colorbar(im, cax=cax)\n",
    "cbar.set_label(\"Output [RPU]\", fontsize=6)\n",
    "cbar.set_ticks([10**(-2), 10**(-1), 10**0, 10**1])\n",
    "# cbar.set_scale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{figure_dir}/cross_reactivity{figure_extension}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cac425-4bf7-4a0a-815a-9f64d1ce3923",
   "metadata": {},
   "source": [
    "## Visualize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54770251-d27b-4f02-8a14-cd49a71cff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (50, 40)\n",
    "linthreshs = [1, 1, 0.1]\n",
    "for construct_id in data_inputs:\n",
    "    cur_data = data_inputs[construct_id]\n",
    "    inducer_concentration = cur_data[\"inducer concentrations\"]\n",
    "    output_RPU = cur_data[\"output\"]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=mm_to_inch(figsize))\n",
    "\n",
    "\n",
    "    if \"model information\" in cur_data:\n",
    "        params_dict = cur_data[\"model information\"][\"parameters\"]\n",
    "        X = histogram_bins\n",
    "        params = [params_dict[param_id] for param_id in [\"y_max\", \"y_min\", \"n\", \"k\"]]\n",
    "        Y_model = activatory_hill_equation(X, params)\n",
    "        ax.plot(X, Y_model, color=\"#808080\", label=\"Model\", zorder=-5)\n",
    "        \n",
    "\n",
    "    \n",
    "    ax.scatter(inducer_concentration, list(map(np.median, output_RPU)), color=COLORS_MAIN[0], s=10) \n",
    "    masks = np.array(cur_data[\"replicates merge info\"], dtype=bool)\n",
    "    for iR, replicate_id in enumerate(cur_data[\"replicates\"]):\n",
    "        replicate_output_RPU = cur_data[\"replicates\"][replicate_id][\"output\"]                \n",
    "        Y = list(map(np.median, [elem for flag, elem in zip(masks[:, iR], replicate_output_RPU) if flag]))        \n",
    "        ax.scatter(np.array(inducer_concentration)[masks[:, iR]], Y, color=\"k\", marker=\".\", s=3) \n",
    "        Y = list(map(np.median, [elem for flag, elem in zip(masks[:, iR], replicate_output_RPU) if not flag]))\n",
    "        ax.scatter(np.array(inducer_concentration)[np.logical_not(masks[:, iR])], Y, color=COLOR_GRAY, marker=\".\", s=3) \n",
    "    # ax.plot(inducer_concentration,  list(map(np.median, output_RPU)), color=COLORS_MAIN[0])\n",
    "\n",
    "\n",
    "    linthresh = 10**np.floor(np.log10(inducer_concentration[1] / 4))\n",
    "    \n",
    "    ax.set_xlim((-linthresh/2, np.max(inducer_concentration) * 2))\n",
    "    ax.set_ylim((lim[0], 10**1.4))\n",
    "    ax.set_xlabel(f\"{cur_data['inducer molecule']} [{cur_data['inducer unit']}]\")\n",
    "    ax.set_ylabel(\"Output [RPU]\")    \n",
    "    ax.set_xscale(\"symlog\", linthresh=linthresh)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.tick_params(axis='both', which='both', direction=\"in\", bottom=True, left=True)\n",
    "    # ax.set_title(construct_id)\n",
    "\n",
    "    plt.savefig(figure_dir + \"input_\" + construct_id + \"_response_curve\" + figure_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a714877-4a5f-4647-a923-65c91cf83125",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (180, 60)\n",
    "\n",
    "cur_colors = [\"#EB99B4\", \"#F5B199\", \"#FFCC99\"]\n",
    "for construct_id in data_inputs:\n",
    "    cur_data = data_inputs[construct_id]\n",
    "    inducer_concentrations = cur_data[\"inducer concentrations\"]\n",
    "    output_RPU = cur_data[\"output\"]\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=len(inducer_concentrations), sharey=True, figsize=mm_to_inch(figsize))\n",
    "    fig.subplots_adjust(wspace=0)\n",
    "\n",
    "    is_first = True\n",
    "    iL = 0\n",
    "    \n",
    "    masks = np.array(cur_data[\"replicates merge info\"])\n",
    "    for ax, ind_level, out_levels in zip(axes, inducer_concentrations, output_RPU):    \n",
    "            \n",
    "        ax.hist(out_levels, \n",
    "                orientation=\"horizontal\", \n",
    "                color=COLORS_MAIN[0], \n",
    "                bins=histogram_bins, \n",
    "                density=hist_as_density,\n",
    "                histtype=\"stepfilled\",\n",
    "                linewidth=0.4,\n",
    "                ec=\"k\")\n",
    "\n",
    "        for iR, replicate_id in enumerate(cur_data[\"replicates\"]):            \n",
    "            replicate_output_RPU = cur_data[\"replicates\"][replicate_id][\"output\"]        \n",
    "            color = COLOR_REPLICATES[iR]\n",
    "            alpha = 0.4 if masks[iL, iR] else 0.2\n",
    "                \n",
    "            ax.hist(replicate_output_RPU[iL], \n",
    "                    orientation=\"horizontal\", \n",
    "                    color=color, \n",
    "                    bins=histogram_bins, \n",
    "                    density=hist_as_density, \n",
    "                    alpha=alpha,\n",
    "                    histtype=\"stepfilled\",\n",
    "                    linewidth=0.2,\n",
    "                   )\n",
    "            if True and not masks[iL, iR]:\n",
    "                ax.hist(replicate_output_RPU[iL], \n",
    "                    orientation=\"horizontal\", \n",
    "                    color=COLOR_GRAY,#color, \n",
    "                    bins=histogram_bins, \n",
    "                    density=hist_as_density, \n",
    "                    alpha=0.2,\n",
    "                    histtype=\"stepfilled\",\n",
    "                    linewidth=0.2,\n",
    "                    )    \n",
    "\n",
    "        xlim = ax.get_xlim()\n",
    "        ax.set_xlim((xlim[0], xlim[1] * 1.2))\n",
    "        ax.set_yscale(\"log\")\n",
    "        # ax.set_title(inducer_concentrations)\n",
    "        ax.set_xticks([])  \n",
    "\n",
    "        if is_first:\n",
    "           ax.set_ylabel(\"Output [RPU]\")\n",
    "           ax.tick_params(axis='y', which='both', direction=\"in\", left=True)            \n",
    "           is_first = False\n",
    "        else:\n",
    "           ax.tick_params(axis='y', which='both', left=False)    \n",
    "            \n",
    "        iL += 1\n",
    "        ax.set_xlabel(f\"{ind_level}\")\n",
    "        \n",
    "    plt.suptitle(construct_id)\n",
    "    plt.savefig(figure_dir + \"input_\" + construct_id + \"_histograms\" + figure_extension)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcfef35-bfb6-4e6a-a579-6221a16a4feb",
   "metadata": {},
   "source": [
    "## Visualize Gates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07728a9-893b-4742-971b-db183e0b695f",
   "metadata": {},
   "source": [
    "### Visualization of Individual Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa04c6e-6ac2-459b-91f1-3940c2cb5579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "figsize = (48, 50)\n",
    "figsize = (88, 86)\n",
    "# figsize = (120, 120)\n",
    "for construct_id in data_gates:\n",
    "    cur_data = data_gates[construct_id]\n",
    "    input_RPU = cur_data[\"input RPU\"]\n",
    "    output_RPU = cur_data[\"output\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=mm_to_inch(figsize))\n",
    "    ax.scatter(input_RPU, list(map(np.median, output_RPU)), color=COLORS_MAIN[0], s=10) \n",
    "    # ax.plot(input_RPU,  list(map(np.median, output_RPU)), color=COLORS_MAIN[0])\n",
    "\n",
    "    if \"model information\" in cur_data:\n",
    "        params_dict = cur_data[\"model information\"][\"parameters\"]\n",
    "        X = np.logspace(-5, 4, 100)#histogram_bins\n",
    "        params = [params_dict[param_id] for param_id in [\"y_max\", \"y_min\", \"n\", \"k\"]]\n",
    "        Y_model = inhibitory_hill_equation(X, params)\n",
    "        ax.plot(X, Y_model, color=COLORS_MAIN[1], label=\"Model\")\n",
    "        \n",
    "\n",
    "    masks = np.array(cur_data[\"replicates merge info\"], dtype=bool)\n",
    "    for iR, replicate_id in enumerate(cur_data[\"replicates\"]):\n",
    "        replicate_output_RPU = cur_data[\"replicates\"][replicate_id][\"output\"]        \n",
    "        Y = list(map(np.median, [elem for flag, elem in zip(masks[:, iR], replicate_output_RPU) if flag]))\n",
    "        ax.scatter(np.array(input_RPU)[masks[:, iR]], Y, color=\"k\", marker=\".\", s=3) \n",
    "        Y = list(map(np.median, [elem for flag, elem in zip(masks[:, iR], replicate_output_RPU) if not flag]))\n",
    "        ax.scatter(np.array(input_RPU)[np.logical_not(masks[:, iR])], Y, color=COLORS[2], marker=\".\", s=3)  \n",
    "\n",
    "    ax.set_xticks([10**(-3), 10**(-1), 10**1])\n",
    "    ax.set_yticks([10**(-3), 10**(-1), 10**1])    \n",
    "    ax.set_xlim(lim)\n",
    "    ax.set_ylim(lim)\n",
    "    ax.set_xlabel(\"Input [RPU]\")\n",
    "    ax.set_ylabel(\"Output [RPU]\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.tick_params(axis='both', which='both', direction=\"in\", bottom=True, left=True)\n",
    "    ax.set_box_aspect(1)\n",
    "    ax.set_title(construct_id)\n",
    "    plt.savefig(f'{figure_dir}gate_{construct_id}_response_curve_{\"-\".join(list(map(str,figsize)))}{figure_extension}')\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9bd2fe-aa77-41cb-a9ed-eca064434afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (48, 50)\n",
    "# figsize = (88, 86)\n",
    "# figsize = (100, 50)\n",
    "\n",
    "for construct_id in data_gates:\n",
    "    cur_data = data_gates[construct_id]\n",
    "    inducer_concentrations = cur_data[\"inducer concentrations\"]\n",
    "    input_RPU = cur_data[\"input RPU\"]\n",
    "    output_RPU = cur_data[\"output\"]\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=len(input_RPU), sharey=True, figsize=mm_to_inch(figsize))\n",
    "    fig.subplots_adjust(wspace=0)\n",
    "\n",
    "\n",
    "    is_first = True\n",
    "    iL = 0\n",
    "    \n",
    "    masks = np.array(cur_data[\"replicates merge info\"], dtype=bool)\n",
    "    for ax, ind_level, input_level, out_levels in zip(axes, inducer_concentrations, input_RPU, output_RPU):    \n",
    "            \n",
    "        ax.hist(out_levels, \n",
    "           orientation=\"horizontal\", \n",
    "           color=COLORS_MAIN[0], \n",
    "           bins=histogram_bins, \n",
    "           density=hist_as_density,\n",
    "               histtype=\"stepfilled\",\n",
    "                linewidth=0.4,\n",
    "               ec=\"k\")\n",
    "\n",
    "        for iR, replicate_id in enumerate(cur_data[\"replicates\"]):\n",
    "            replicate_output_RPU = cur_data[\"replicates\"][replicate_id][\"output\"]        \n",
    "            color = COLOR_REPLICATES[iR]\n",
    "            alpha = 0.4 if masks[iL, iR] else 0.2\n",
    "                \n",
    "            ax.hist(replicate_output_RPU[iL], \n",
    "                    orientation=\"horizontal\", \n",
    "                    color=color, \n",
    "                    bins=histogram_bins, \n",
    "                    density=hist_as_density, \n",
    "                    alpha=alpha,\n",
    "                    histtype=\"stepfilled\",\n",
    "                    linewidth=0.2,\n",
    "                   )\n",
    "            if True and not masks[iL, iR]:\n",
    "                ax.hist(replicate_output_RPU[iL], \n",
    "                    orientation=\"horizontal\", \n",
    "                    color=COLOR_GRAY,#color, \n",
    "                    bins=histogram_bins, \n",
    "                    density=hist_as_density, \n",
    "                    alpha=0.2,\n",
    "                    histtype=\"stepfilled\",\n",
    "                    linewidth=0.2,\n",
    "                    )    \n",
    "\n",
    "        xlim = ax.get_xlim()\n",
    "        ax.set_xlim((xlim[0], xlim[1] * 1.2))\n",
    "        ax.set_yscale(\"log\")\n",
    "        # ax.set_xlabel(f\"{int(ind_level)}\", fontsize=6)#:.3f}\")\n",
    "        ax.set_xticks([])  \n",
    "\n",
    "        if is_first:\n",
    "            ax.set_ylabel(\"Output [RPU]\")\n",
    "            ax.tick_params(axis='y', which='both', direction=\"in\", left=True)            \n",
    "            is_first = False\n",
    "        else:\n",
    "            ax.tick_params(axis='y', which='both', left=False)            \n",
    "        iL += 1\n",
    "            \n",
    "    plt.suptitle(construct_id)\n",
    "    plt.savefig(f\"{figure_dir}gate_{construct_id}_histograms_{'-'.join(list(map(str,figsize)))}{figure_extension}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ec815-30d2-4bd7-9718-09aeebd57d84",
   "metadata": {},
   "source": [
    "### Visualization of Gate Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a05ca-274b-4830-94a7-b34d01e28aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_id_median_pairs = [(gate_id, list(map(np.median, data_gates[gate_id][\"output\"]))) for gate_id in data_gates]\n",
    "gate_dynamic_ranges = [(elem[0], np.nanmax(elem[1])/np.nanmin(elem[1])) for elem in gate_id_median_pairs]\n",
    "gate_dynamic_ranges = sorted(gate_dynamic_ranges, key=lambda elem: elem[1], reverse=True)\n",
    "gate_dynamic_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc49de-50ed-46b6-9d59-f7bac889b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "figsize = (88, 86)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=mm_to_inch(figsize))\n",
    "for iX, gate_tup in enumerate(gate_dynamic_ranges):\n",
    "    if iX >= present_top_k:\n",
    "        break\n",
    "    construct_id, dr = gate_tup\n",
    "    cur_data = data_gates[construct_id]\n",
    "    input_RPU = cur_data[\"input RPU\"]\n",
    "    output_RPU = cur_data[\"output\"]\n",
    "\n",
    "#    if \"model information\" in cur_data:\n",
    "    params_dict = cur_data[\"model information\"][\"parameters\"]\n",
    "    X = np.logspace(-5, 4, 100)#histogram_bins\n",
    "    params = [params_dict[param_id] for param_id in [\"y_max\", \"y_min\", \"n\", \"k\"]]\n",
    "    Y_model = inhibitory_hill_equation(X, params)\n",
    "    ax.plot(X, Y_model, color=COLORS[iX], label=f\"{construct_id}\")\n",
    "\n",
    "    \n",
    "    # ax.scatter(input_RPU, list(map(np.median, output_RPU)), color=COLORS[iX], label=construct_id, s=10) \n",
    "    # ax.plot(input_RPU,  \n",
    "    #         list(map(np.median, output_RPU)), \n",
    "    #         [\"--\", \"-.\", \"--.\"][iX % 3], \n",
    "    #         color=(COLORS + COLORS_DARK)[(iX) % (len(COLORS) + len(COLORS_DARK) - 1)],\n",
    "    #         label=construct_id)\n",
    "\n",
    "ax.set_xlabel(\"Input [RPU]\")\n",
    "ax.set_ylabel(\"Output [RPU]\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")            \n",
    "\n",
    "ax.tick_params(axis='x', which='both', direction=\"in\", left=True)   \n",
    "ax.tick_params(axis='y', which='both', direction=\"in\", left=True)            \n",
    "ax.set_box_aspect(1)         \n",
    "\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "        \n",
    "ax.legend()\n",
    "plt.savefig(figure_dir +  \"gates_all_response_curves\" + figure_extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8799f06e-b1e4-4483-a577-843274734eef",
   "metadata": {},
   "source": [
    "### Gate Library Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e12df-1d60-4cc2-a2c7-5ca1c5b5dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rep = []\n",
    "\n",
    "params_order = [\"y_max\", \"y_min\", \"n\", \"k\"]\n",
    "header = [\"Plasmid\", \"Gate\"] + params_order + [\"Dynamic Range\"]\n",
    "table_rep.append(header)\n",
    "\n",
    "for construct_id in data_inputs:\n",
    "    cur_data = data_inputs[construct_id]\n",
    "    \n",
    "    params_dict = cur_data[\"model information\"][\"parameters\"]\n",
    "    params = [params_dict[param_id] for param_id in params_order]        \n",
    "    \n",
    "    \n",
    "    row_vals = [construct_id, \"\"]\n",
    "    row_vals += [f\"{val:.3f}\" for val in params]\n",
    "    row_vals += [f\"{params[0] / params[1] :.3f}\"]\n",
    "    \n",
    "    table_rep.append(row_vals)\n",
    "    \n",
    "    \n",
    "table_str = \"\\n\".join([\", \".join(row) for row in table_rep])\n",
    "print(table_rep)\n",
    "with open(f\"{figure_dir}/input_hill_parameters.csv\", \"w\") as file:\n",
    "    file.write(table_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8f197-fcf8-437a-b04c-1bbe618f31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rep = []\n",
    "\n",
    "params_order = [\"y_max\", \"y_min\", \"n\", \"k\"]\n",
    "header = [\"Plasmid\", \"Gate\"] + params_order + [\"Dynamic Range\"]\n",
    "table_rep.append(header)\n",
    "\n",
    "for construct_id in data_gates:\n",
    "    cur_data = data_gates[construct_id]\n",
    "    \n",
    "    params_dict = cur_data[\"model information\"][\"parameters\"]\n",
    "    params = [params_dict[param_id] for param_id in params_order]        \n",
    "    \n",
    "    \n",
    "    row_vals = [construct_id, \"\"]\n",
    "    row_vals += [f\"{val:.3f}\" for val in params]\n",
    "    row_vals += [f\"{params[0] / params[1] :.3f}\"]\n",
    "    \n",
    "    table_rep.append(row_vals)\n",
    "    \n",
    "    \n",
    "table_str = \"\\n\".join([\", \".join(row) for row in table_rep])\n",
    "print(table_rep)\n",
    "with open(f\"{figure_dir}/gate_parameters.csv\", \"w\") as file:\n",
    "    file.write(table_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
