{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "251cf0af",
   "metadata": {},
   "source": [
    "<H1>Prepare Data</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8fd762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import FlowCal as fcs\n",
    "import os\n",
    "from scipy.stats import gaussian_kde\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "\n",
    "\n",
    "data_dir = \"data/\"\n",
    "for rep_dir in os.listdir(data_dir):\n",
    "    rep_dir_path = os.path.join(data_dir, rep_dir)\n",
    "    if not os.path.isdir(rep_dir_path):\n",
    "        continue    \n",
    "    if not rep_dir.startswith(\"replicate\"):\n",
    "        continue\n",
    "    \n",
    "    for const_dir in os.listdir(rep_dir_path):\n",
    "        const_dir_path = os.path.join(rep_dir_path, const_dir)\n",
    "        if not os.path.isdir(const_dir_path):\n",
    "            continue   \n",
    "\n",
    "        target_dir = os.path.join(data_dir, const_dir, rep_dir)\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        for file in os.listdir(const_dir_path):\n",
    "            file_path = os.path.join(const_dir_path, file)\n",
    "            if not os.path.isfile(file_path):\n",
    "                continue\n",
    "\n",
    "            target_file_path = os.path.join(target_dir, file)\n",
    "            shutil.copyfile(file_path, target_file_path)\n",
    "            print(f\"Copied {file_path} to {target_file_path}\")\n",
    "\n",
    "data_dir_constitutive = \"data/constitutive/\"\n",
    "data_dir_inputs = \"data/inputs/\"\n",
    "data_dir_gates = \"data/gates/\"\n",
    "data_dir_reference = \"data/reference/\"\n",
    "data_dir_basal = \"data/basal/\"\n",
    "data_dir_inputs_cross_reactivity = \"data/inputs_cross_reactivity/\"\n",
    "\n",
    "figure_dir = \"figures/\"\n",
    "figure_extension = \".pdf\"\n",
    "\n",
    "os.makedirs(figure_dir, exist_ok=True)\n",
    "\n",
    "reference_promoter = \"J23101\"                                           # The constitutive reference promoter which serves as baseline for the RPU conversion\n",
    "alternative_reference_promoter_name = {\"J23101\": \"pJCM434\"}             # Currently, the reference promoter is part of the dataset in two different names\n",
    "inducer_units = {\"ind1\": \"pgul\", \"ind2\": \"uM\", \"ind3\": \"uM\"}\n",
    "inducer_molecule_type = {\"pJCM435\": \"IPTG\", \"pJCM448\": \"Ara\", \"pJCM449\": \"aTc\"}\n",
    "controlling_input_sensor = \"pJCM435\"                                    # The input sensor used for the characterization of the gate plasmids\n",
    "facs_channel = \"FL1-A\"                                                  # The fluorescence FACS channel we are interested in\n",
    "p_gating = 0.95                                                         # The probability mass to preserve after gating\n",
    "gating_channels = [\"FSC-A\", \"FSC-H\"]                                    # The two channels to apply gating on\n",
    "kde_bandwith = 0.05\n",
    "\n",
    "\n",
    "n_walkers = 10\n",
    "n_chains = 10\n",
    "n_samples = 10000\n",
    "\n",
    "\n",
    "lim = (10**(-3), 10**2)\n",
    "histogram_bins = np.logspace(np.log10(lim[0]), np.log10(lim[1]), 200)                \n",
    "hist_as_density = False                                                # Whether to turn the histogram into a valid density (sum over width x height = 1) or not\n",
    "plot_as_density = False                                                 # Whether to use kernel density estimate instead of histograms for representing distributions\n",
    "\n",
    "input_sensor_order = [\"Ptac\", \"PBAD\", \"Ptet\"]\n",
    "inducer_order = [\"No\", \"IPTG\", \"Ara\", \"aTc\"]\n",
    "present_top_k = 5\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "font = {'family': 'serif',\n",
    "        'serif': ['Helvetica'],\n",
    "        'size': 8}\n",
    "\n",
    "savefig = {'bbox': 'tight',\n",
    "           'pad_inches': 0.01,\n",
    "           'dpi': 1200,\n",
    "           'transparent': True}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('savefig', **savefig)\n",
    "mm_to_inch = lambda val: np.array(val) * 0.0393701\n",
    "\n",
    "COLORS = [(0.30, 0.56, 1.00), (0.35, 0.24, 1.00), (1.00, 0.00, 0.42), (1.00, 0.40, 0.10), (1.00, 0.65, 0.19)]\n",
    "COLORS_DARK = [(0.00, 0.24, 0.90), (0.28, 0.00, 0.84), (0.80, 0.00, 0.27), (0.90, 0.24, 0.00), (1.00, 0.50, 0.00)]\n",
    "COLORS_MAIN = [(0.30, 0.55, 1.00), (1.00, 0.40, 0.10)]\n",
    "COLORS_CMAP_ORANGE = [\"#FFFFFF\", \"#FF5500\", \"#B3003C\"]\n",
    "COLORS_CMAP_BLUE = [\"#FFFFFF\", \"#69A3FF\", \"#4400D6\"]\n",
    "\n",
    "COLORMAP_ORANGE = LinearSegmentedColormap.from_list(\"my_cmap\", COLORS_CMAP_ORANGE)\n",
    "COLORMAP_BLUE = LinearSegmentedColormap.from_list(\"my_cmap\", COLORS_CMAP_BLUE)\n",
    "\n",
    "COLOR_GRAY = \"#808080\"\n",
    "COLOR_REPLICATES = COLORS_DARK[2:] # Use only redish colors for replicates\n",
    "\n",
    "\n",
    "def list_facs_files(directory):\n",
    "    replicate_directories = os.listdir(directory)                    # Gets all the elements in data_dir and stores them as list in files \n",
    "\n",
    "    relevant_files = []                             # Target list to store the relevant files in\n",
    "    for rep_dir in replicate_directories:\n",
    "        rep_dir_path = os.path.join(directory, rep_dir)\n",
    "        if not os.path.isdir(rep_dir_path):\n",
    "            continue\n",
    "            \n",
    "        files = os.listdir(rep_dir_path)\n",
    "        # We here filter for .fcs files and store them in relevant_files        \n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[-1] != \".fcs\":    # Exclude not .fcs files            \n",
    "                continue\n",
    "            relevant_files.append(os.path.join(rep_dir_path, file))                 # Adds file to the list of relevant files\n",
    "            # break\n",
    "    return relevant_files\n",
    "\n",
    "def merge_replicates(replicates_data):\n",
    "    # Compare replicates via median values and discard whole dataset in case the replicates deviation is too large.    \n",
    "    merged_data = None\n",
    "    num_replicates = len(replicates_data)\n",
    "\n",
    "    if num_replicates < 1:\n",
    "        return merged_data\n",
    "        \n",
    "    num_levels = len(replicates_data[0])\n",
    "\n",
    "    merged_data = [None] * num_levels\n",
    "    merged_info = [None] * num_levels\n",
    "    for iL in range(num_levels):\n",
    "        dist_mat = np.zeros(shape=(num_replicates, num_replicates))\n",
    "        print(\"Merge\", iL, list(map(np.median, [replicate[iL] for replicate in replicates_data])))\n",
    "        replicate_lengths = np.array([len(replicate[iL]) for replicate in replicates_data])\n",
    "        replicates_to_consider = replicate_lengths >= 1000     # Require at least 1000 samples\n",
    "        \n",
    "        for iR1 in range(num_replicates):            \n",
    "            rep_val_1 = np.median(replicates_data[iR1][iL])            \n",
    "            for iR2 in range(num_replicates):\n",
    "                if iR2 < iR1:\n",
    "                    continue\n",
    "                    \n",
    "                rep_val_2 = np.median(replicates_data[iR2][iL])\n",
    "                dist_mat[iR1, iR2] = rep_val_1 / rep_val_2\n",
    "                dist_mat[iR2, iR1] = dist_mat[iR1, iR2]\n",
    "        \n",
    "        relative_distances = np.abs(np.log2(dist_mat))\n",
    "        vals = np.triu(relative_distances, k=1)\n",
    "        threshold = 3\n",
    "        mask = relative_distances <= threshold\n",
    "        use_replicates = np.all(mask[replicates_to_consider][:, replicates_to_consider])      # Only consider replicates with sufficient read counts\n",
    "        indices_to_consider = np.logical_and(replicates_to_consider, use_replicates)\n",
    "        \n",
    "            \n",
    "        print(\"Merge\", iL, indices_to_consider)\n",
    "        cur_data = [replicates_data[iR][iL] for iR, flag in enumerate(indices_to_consider) if flag]\n",
    "        if len(cur_data) > 0:\n",
    "            merged_data[iL] = np.concatenate(cur_data)\n",
    "        else:\n",
    "            merged_data[iL] = np.array([])\n",
    "        merged_info[iL] = indices_to_consider\n",
    "    return merged_data, merged_info\n",
    "                \n",
    "    \n",
    "\n",
    "def transform_data_dict(data_dict, construct_inducer_units={}):\n",
    "    for construct_id in data_dict:\n",
    "        print(construct_id)\n",
    "        replicates = {}\n",
    "        for replicate_id in data_dict[construct_id]:\n",
    "            cur_data = data_dict[construct_id][replicate_id]\n",
    "            replicate_inducer_unit = \"\"\n",
    "            replicate_inducer_molecule = \"\"\n",
    "            if construct_id in construct_inducer_units:\n",
    "                replicate_inducer_unit = construct_inducer_units[construct_id][\"unit\"]\n",
    "                replicate_inducer_molecule = construct_inducer_units[construct_id][\"molecule\"]\n",
    "                \n",
    "            replicate_inducer_concentrations = list(cur_data.keys())\n",
    "            replicate_inducer_concentrations = sorted(replicate_inducer_concentrations)            \n",
    "            \n",
    "            fcs_datasets = [cur_data[conc] for conc in replicate_inducer_concentrations]\n",
    "            replicate_data = [fcs_data[:, facs_channel] for fcs_data in fcs_datasets]\n",
    "            replicates[replicate_id] = {\"inducer unit\": replicate_inducer_unit,\n",
    "                                        \"inducer molecule\": replicate_inducer_molecule,\n",
    "                                        \"inducer concentrations\": replicate_inducer_concentrations, \n",
    "                                        \"data\": replicate_data}\n",
    "\n",
    "        replicate_inducer_units = [replicates[rep_id][\"inducer unit\"] for rep_id in replicates]\n",
    "        replicate_inducer_molecules = [replicates[rep_id][\"inducer molecule\"] for rep_id in replicates]\n",
    "        if len(set(replicate_inducer_units)) > 1:\n",
    "            raise Exception(\"Replicates have different inducer units\")\n",
    "        if len(set(replicate_inducer_molecules)) > 1:\n",
    "            raise Exception(\"Replicates have different inducer molecules\")\n",
    "            \n",
    "        inducer_unit = replicate_inducer_units[0]\n",
    "        inducer_molecule = replicate_inducer_molecules[0]\n",
    "\n",
    "        replicate_inducer_concentrations = [replicates[rep_id][\"inducer concentrations\"] for rep_id in replicates]\n",
    "        if not all([elem == replicate_inducer_concentrations[0] for elem in replicate_inducer_concentrations]):\n",
    "            raise Exception(\"Not all replicates have the same inducer concentrations\")\n",
    "        \n",
    "        inducer_concentrations = replicate_inducer_concentrations[0]\n",
    "\n",
    "        replicates_data = [replicates[rep_id][\"data\"] for rep_id in replicates]\n",
    "        merged_data, merged_info = merge_replicates(replicates_data)        \n",
    "        data = merged_data\n",
    "        \n",
    "        data_dict[construct_id] = {\"inducer unit\": inducer_unit,\n",
    "                                   \"inducer molecule\": inducer_molecule,\n",
    "                                   \"inducer concentrations\": inducer_concentrations,\n",
    "                                   \"data unit\": \"AU\",\n",
    "                                   \"data\": data,\n",
    "                                   \"replicates merge info\": merged_info, \n",
    "                                   \"replicates\": replicates}\n",
    "\n",
    "def load_constitutive_data(file_paths):\n",
    "    data_dict = {}             # Dictionary for storing the data with the inducer level as key\n",
    "    for path in file_paths:                                              # Iterate over the facs files\n",
    "        facs_data = fcs.io.FCSData(path)                                 # Use library to load facs data    \n",
    "        facs_data = fcs.gate.density2d(facs_data,                        # Gate facs data\n",
    "                                           channels=gating_channels,\n",
    "                                           gate_fraction=p_gating)       \n",
    "        # Extract and parse file name\n",
    "        replicate_id = os.path.basename(os.path.split(path)[0])\n",
    "        filename = os.path.basename(path)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        if not name in data_dict:\n",
    "            data_dict[name] = {}\n",
    "        data_dict[name][replicate_id] = {np.nan: facs_data}\n",
    "        print(f\"Loaded data for `{name}` ({replicate_id})\")\n",
    "\n",
    "\n",
    "    transform_data_dict(data_dict)\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def load_inducible_data(file_paths):\n",
    "    \n",
    "    data_dict = {}             # Dictionary for storing the data with the inducer level as key\n",
    "    construct_inducer_units = {}\n",
    "    \n",
    "    for path in file_paths:                                              # Iterate over the facs files\n",
    "        facs_data = fcs.io.FCSData(path)                                 # Use library to load facs data    \n",
    "        facs_data = fcs.gate.density2d(facs_data,                        # Gate facs data\n",
    "                                           channels=gating_channels,\n",
    "                                           gate_fraction=p_gating)\n",
    "        \n",
    "        # Extract and parse file name\n",
    "        replicate_id = os.path.basename(os.path.split(path)[0])\n",
    "        filename = os.path.basename(path)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "    \n",
    "        # Extract and parse file name\n",
    "        # Exemplary content of filename: pJCM435 0uM.fcs\n",
    "        # Format is: \"[construct_id] [inducer_level][inducer_unit].fcs\"\n",
    "        construct_id, inducer_description = name.split(\" \")\n",
    "        \n",
    "        inducer_unit = None        \n",
    "        for unit in inducer_units.values():      \n",
    "            \n",
    "            if unit in inducer_description:\n",
    "                inducer_unit = unit\n",
    "        if inducer_unit is None:\n",
    "            raise Exception(f\"Unknow inducer unit in {inducer_description}\")\n",
    "    \n",
    "        inducer_concentration = float(inducer_description.replace(inducer_unit, \"\"))\n",
    "        \n",
    "        if not construct_id in data_dict:\n",
    "            data_dict[construct_id] = {}\n",
    "            construct_inducer_units[construct_id] = {\"molecule\": inducer_molecule_type[construct_id] if construct_id in inducer_molecule_type else \"\", \n",
    "                                                     \"unit\": inducer_unit}\n",
    "            \n",
    "        if not replicate_id in data_dict[construct_id]:\n",
    "            data_dict[construct_id][replicate_id] = {}\n",
    "        \n",
    "        data_dict[construct_id][replicate_id][inducer_concentration] = facs_data\n",
    "        \n",
    "        print(f\"Loaded data for construct `{construct_id}` and inducer concentration {inducer_concentration} {inducer_unit} ({replicate_id})\")\n",
    "    \n",
    "    \n",
    "    transform_data_dict(data_dict, construct_inducer_units)\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "# We here infer the inducer concentration from the file names and load the facs data\n",
    "\n",
    "file_paths = list_facs_files(data_dir_constitutive)\n",
    "file_paths = sorted(file_paths)\n",
    "# file_paths = [path for path in file_paths if \"J23107\" in path]   # Exemplary code for loading data of a single construt only\n",
    "\n",
    "data_constitutive = load_constitutive_data(file_paths)\n",
    "rpu_reference_data = data_constitutive[reference_promoter + \"-No\"][\"data\"]\n",
    "\n",
    "# We here infer the inducer concentration from the file names and load the facs data\n",
    "\n",
    "file_paths = list_facs_files(data_dir_inputs)\n",
    "file_paths = sorted(file_paths)\n",
    "\n",
    "data_inputs = load_inducible_data(file_paths)\n",
    "\n",
    "file_paths = list_facs_files(data_dir_gates)\n",
    "file_paths = sorted(file_paths)\n",
    "# file_paths = [path for path in file_paths if \"pJCM212\" in path] # Exemplary code for loading data of a single gate construct\n",
    "\n",
    "data_gates = load_inducible_data(file_paths)\n",
    "\n",
    "# We here infer the inducer concentration from the file names and load the facs data\n",
    "\n",
    "file_paths = list_facs_files(data_dir_reference)\n",
    "file_paths = sorted(file_paths)\n",
    "\n",
    "data_reference = load_inducible_data(file_paths)\n",
    "ref_name = alternative_reference_promoter_name[reference_promoter]\n",
    "\n",
    "file_paths = list_facs_files(data_dir_basal)\n",
    "file_paths = sorted(file_paths)\n",
    "\n",
    "data_basal = load_inducible_data(file_paths)\n",
    "autofluorescence_reference_data = np.concatenate((data_basal[\"DH10B\"][\"data\"])) # Use data from all inducer concentrations as reference\n",
    "\n",
    "# We here infer the inducer concentration from the file names and load the facs data\n",
    "\n",
    "file_paths = list_facs_files(data_dir_inputs_cross_reactivity)\n",
    "file_paths = sorted(file_paths)\n",
    "\n",
    "data_inputs_cross_reactivity = load_constitutive_data(file_paths)\n",
    "\n",
    "\n",
    "def transform_to_RPU(data_dict, rpu_reference_data, autofluorescence_reference_data):\n",
    "    def rpu_transform(data, FP_0, FP_RPU):\n",
    "        output_data = [None] * len(data)\n",
    "        for iX in range(len(data)):\n",
    "            FP = np.median(data[iX])\n",
    "            \n",
    "            c = (FP - FP_0)/FP * 1 / (FP_RPU - FP_0)\n",
    "            \n",
    "            output_data[iX]= data[iX] * c        \n",
    "        return output_data\n",
    "        \n",
    "    FP_RPU = np.median(rpu_reference_data)\n",
    "    FP_0 = np.median(autofluorescence_reference_data)\n",
    "    # print(FP_RPU)\n",
    "    for construct_id in data_dict:\n",
    "        cur_entry = data_dict[construct_id]\n",
    "  \n",
    "            \n",
    "        data = cur_entry[\"data\"]        \n",
    "        cur_entry[\"output\"] = rpu_transform(data, FP_0, FP_RPU)\n",
    "        if \"replicates\" in cur_entry:\n",
    "            for replicate_id in cur_entry[\"replicates\"]:\n",
    "                data =  cur_entry[\"replicates\"][replicate_id][\"data\"]        \n",
    "                cur_entry[\"replicates\"][replicate_id][\"output\"] = rpu_transform(data, FP_0, FP_RPU)\n",
    "\n",
    "\n",
    "transform_to_RPU(data_constitutive, rpu_reference_data, autofluorescence_reference_data)\n",
    "transform_to_RPU(data_inputs, rpu_reference_data, autofluorescence_reference_data)\n",
    "transform_to_RPU(data_gates, rpu_reference_data, autofluorescence_reference_data)\n",
    "transform_to_RPU(data_reference, rpu_reference_data, autofluorescence_reference_data)\n",
    "transform_to_RPU(data_basal, rpu_reference_data, autofluorescence_reference_data)\n",
    "transform_to_RPU(data_inputs_cross_reactivity, rpu_reference_data, autofluorescence_reference_data)\n",
    "\n",
    "\n",
    "def insert_input_rpu(data_dict, input_data):\n",
    "    median_input_RPU = [np.median(elem) for elem in input_data[\"output\"]]\n",
    "    for construct_id in data_dict:\n",
    "        data_dict[construct_id][\"input RPU\"] = median_input_RPU  \n",
    "        # Theoretically, one could assign the RPU histograms instead of the median value.\n",
    "        # However, the combined data wouldn't represent actuall joint data as the data has been generated in two separate experiments.\n",
    "\n",
    "input_data = data_inputs[controlling_input_sensor]\n",
    "\n",
    "insert_input_rpu(data_gates, input_data=input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3b3d21",
   "metadata": {},
   "source": [
    "<H1> Define Models</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df14bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activatory_hill_equation(x, params):\n",
    "    ymax, ymin, n, k = params\n",
    "    y = ymin + (ymax - ymin) / (1 + np.power(k/x, n))\n",
    "    return y\n",
    "\n",
    "def inhibitory_hill_equation(x, params):\n",
    "    ymax, ymin, n, k = params\n",
    "    y = ymin + (ymax - ymin) / (1 + np.power(x/k, n))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c2962",
   "metadata": {},
   "source": [
    "<H1>Define Loss Function</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b84d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_function(model, data):\n",
    "    def loss_function(params):\n",
    "        X, Y_true = data\n",
    "        losses = []\n",
    "        model_params = np.exp(params)        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            Y_pred = model(X, model_params)        \n",
    "        diffs = np.log(Y_pred) - np.log(Y_true)         \n",
    "        \n",
    "        losses = np.power(diffs, 2)  \n",
    "        nan_mask = np.logical_not(np.isnan(losses))\n",
    "        loss = np.sum(losses[nan_mask])        \n",
    "        \n",
    "        return loss\n",
    "        \n",
    "\n",
    "    return loss_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489ea54b",
   "metadata": {},
   "source": [
    "<H1>Define Optimizer</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e94046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class OptimizationAlgorithm:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        raise Exception(\"Needs to be implemented by sub class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f60fdc",
   "metadata": {},
   "source": [
    "<H3>Parallel Tempering</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74887530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelTempering(OptimizationAlgorithm):\n",
    "\n",
    "    def __init__(self, log_likelihood, log_prior, n_dim, n_walkers=1, n_chains=10):\n",
    "        self.log_likelihood = log_likelihood\n",
    "        self.log_prior = log_prior\n",
    "        self.n_dim = n_dim\n",
    "\n",
    "        self.n_walkers = n_walkers\n",
    "        self.n_chains = n_chains\n",
    "\n",
    "        swap_mask = np.zeros(shape=(n_walkers, int(np.ceil(n_chains / 2) * 2)), dtype=bool)\n",
    "        swap_mask[:, ::2] = 1\n",
    "        self.swap_mask = swap_mask\n",
    "        pass\n",
    "\n",
    "    def run(self, initial_parameters, n_samples=10 ** 3, target_acceptance_ratio=None,\n",
    "            adaptive_temperature=True):\n",
    "\n",
    "        n_walkers = self.n_walkers\n",
    "        n_chains = self.n_chains\n",
    "\n",
    "        initial_parameters = np.array(initial_parameters)\n",
    "        self.temperatures = np.power(2, np.arange(self.n_chains), dtype=float)\n",
    "\n",
    "        if adaptive_temperature and n_chains <= 2:\n",
    "            print(\n",
    "                f\"Disabling adaptive temperature for n_chains={n_chains}. Minimal number of chains for adaptive temperature is 3, but more chains are recommended.\")\n",
    "            adaptive_temperature = False\n",
    "\n",
    "        if adaptive_temperature:\n",
    "            self.temperatures[-1] = np.inf\n",
    "            # Value choice follows Vousden et al. 2016\n",
    "            v_factor = 10 ** 2\n",
    "            v = int(np.ceil(v_factor / n_walkers))\n",
    "            t0 = 10 * v  \n",
    "            S = np.log(np.diff(self.temperatures, axis=-1))\n",
    "            S = S[:-1]\n",
    "            # Diffs of T_2 - T_1, ..., T_(N-1) - T_(N-2). The diff T_N - T_(N-1) is excluded by purpose following 1 < i < N for the S_i\n",
    "\n",
    "        variance = 0.1\n",
    "        self.variance = np.ones(shape=(self.n_walkers, self.n_chains, self.n_dim))\n",
    "        self.variance = self.variance * np.expand_dims(np.expand_dims(np.arange(1, self.n_chains + 1), axis=0),\n",
    "                                                       axis=-1)\n",
    "        self.variance *= variance\n",
    "\n",
    "        adaptive_proposal_distribution = target_acceptance_ratio is not None and target_acceptance_ratio > 0 and target_acceptance_ratio < 1.0\n",
    "\n",
    "        adaptive_temperature_stop_iteration = int(n_samples / 2)\n",
    "\n",
    "        parameters = np.zeros(shape=(n_samples, n_walkers, n_chains, *initial_parameters.shape))\n",
    "        priors = np.zeros(shape=(n_samples, n_walkers, n_chains))\n",
    "        likelihoods = np.zeros(shape=(n_samples, n_walkers, n_chains))\n",
    "        step_accepts = np.zeros(shape=(n_samples, n_walkers, n_chains))\n",
    "        swap_accepts = []\n",
    "\n",
    "        params = np.array(initial_parameters)\n",
    "        likelihood = self.log_likelihood(params)\n",
    "        prior = self.log_prior(params)\n",
    "        # max_iN = 0\n",
    "        for iN in tqdm(range(n_samples)):\n",
    "            self.beta = 1 / np.expand_dims(self.temperatures, axis=0)\n",
    "\n",
    "            params, prior, likelihood, step_accept = self.step(params, prior, likelihood, index=iN)\n",
    "            swap_round = iN % 10 == 9\n",
    "            if swap_round:\n",
    "                params, prior, likelihood, swap_accept = self.swap(params, prior, likelihood, index=iN)\n",
    "                swap_accepts.append(swap_accept)\n",
    "\n",
    "            parameters[iN] = params\n",
    "            priors[iN] = prior\n",
    "            likelihoods[iN] = likelihood\n",
    "            step_accepts[iN] = step_accept\n",
    "\n",
    "            ##################################\n",
    "            # Adaptive Proposal Distribution #\n",
    "            ##################################\n",
    "            if adaptive_proposal_distribution and iN >= 100 and iN % 10 == 0:\n",
    "                # Considers Windowed average of the last 100 steps\n",
    "                acc_rate_deviation = step_accepts[max(iN - 100 + 1, 0):iN + 1] - target_acceptance_ratio\n",
    "                scaling_params = np.exp((np.mean(acc_rate_deviation, axis=0)))\n",
    "                self.variance = self.variance * np.expand_dims(scaling_params, axis=-1)\n",
    "\n",
    "            ###############################\n",
    "            # Adaptive Temperature Ladder #\n",
    "            ###############################\n",
    "\n",
    "            if adaptive_temperature and swap_round and iN > 20 and iN < adaptive_temperature_stop_iteration:\n",
    "                kappa = 1 / v * t0 / (iN + t0)\n",
    "                # Be aware that only every 10th iteration is a swap iteration\n",
    "                rel_accepts = swap_accepts[max(len(swap_accepts) - 100, 0):]  # Select relevant data\n",
    "                swap_acceptance_rate = np.mean(rel_accepts, axis=0)  # Average over multiple samples\n",
    "                swap_acceptance_rate = np.mean(swap_acceptance_rate, axis=0)  # Average over multiple walkers\n",
    "                swap_rate_diff = -np.diff(swap_acceptance_rate, axis=0)  # Compute the diff over the chains\n",
    "                S = S + kappa * swap_rate_diff\n",
    "                temp_diffs = self.temperatures\n",
    "                temp_diffs[1:-1] = np.exp(S)\n",
    "                self.temperatures = np.cumsum(temp_diffs)\n",
    "\n",
    "        parameters = np.array(parameters)\n",
    "        priors = np.array(priors)\n",
    "        likelihoods = np.array(likelihoods)\n",
    "        step_accepts = np.array(step_accepts)\n",
    "        swap_accepts = np.array(swap_accepts)\n",
    "        return parameters, priors, likelihoods, step_accepts, swap_accepts\n",
    "\n",
    "    def step(self, params, prior, likelihood, index):\n",
    "        move = np.random.normal(loc=0, scale=self.variance)\n",
    "        proposal = params + move\n",
    "\n",
    "        proposal_likelihood = self.log_likelihood(proposal)\n",
    "        proposal_prior = self.log_prior(proposal)\n",
    "        proposal_prob = self.beta * proposal_likelihood + proposal_prior\n",
    "\n",
    "        prob = self.beta * likelihood + prior\n",
    "\n",
    "        log_diff = proposal_prob - prob\n",
    "        diff = np.exp(log_diff)\n",
    "        u = np.random.uniform(size=(self.n_walkers, self.n_chains))\n",
    "        accept = u < diff\n",
    "\n",
    "        new_prior = np.where(accept, proposal_prior, prior)\n",
    "        new_likelihood = np.where(accept, proposal_likelihood, likelihood)\n",
    "\n",
    "        params_accepts = np.expand_dims(accept, -1)\n",
    "        new_params = np.where(params_accepts, proposal, params)\n",
    "        return new_params, new_prior, new_likelihood, accept\n",
    "\n",
    "    def swap(self, params, prior, likelihood, index):\n",
    "        log_diff = np.diff(likelihood, axis=-1)\n",
    "        beta_diff = -np.diff(self.beta, axis=-1)\n",
    "\n",
    "        log_criterion = beta_diff * log_diff\n",
    "        criterion = np.exp(log_criterion)\n",
    "        u = np.random.uniform(size=(self.n_walkers, self.n_chains - 1))\n",
    "        # Ensure in the accepts step that a single chain does not swap to both adjacent chains (it should be possible to check this by using np.diff(accept) which should not yield 0 at a position including a 1 in accept\n",
    "        proposed_accept = u < criterion\n",
    "        self.swap_mask = np.roll(self.swap_mask, 1)\n",
    "        swap_mask = self.swap_mask[:, :self.n_chains - 1]\n",
    "        accept = np.logical_and(proposed_accept, swap_mask)\n",
    "        # accept[:, i] defines whether to swap between chain i and i+1.\n",
    "\n",
    "        # swap_matrice_1 is accept matrice with an additional all zeros entry\n",
    "        swap_matrice_1 = np.concatenate((accept, np.zeros((self.n_walkers, 1))), axis=1)\n",
    "        swap_matrice_2 = np.roll(swap_matrice_1, 1, axis=1)\n",
    "\n",
    "        left_rolled_prior = np.roll(prior, -1, axis=1)\n",
    "        right_rolled_prior = np.roll(prior, 1, axis=1)\n",
    "        left_rolled_likelihood = np.roll(likelihood, -1, axis=1)\n",
    "        right_rolled_likelihood = np.roll(likelihood, 1, axis=1)\n",
    "        left_rolled_params = np.roll(params, -1, axis=1)\n",
    "        right_rolled_params = np.roll(params, 1, axis=1)\n",
    "\n",
    "        new_prior = np.where(swap_matrice_1, left_rolled_prior, prior)\n",
    "        new_prior = np.where(swap_matrice_2, right_rolled_prior, new_prior)\n",
    "        new_likelihood = np.where(swap_matrice_1, left_rolled_likelihood, likelihood)\n",
    "        new_likelihood = np.where(swap_matrice_2, right_rolled_likelihood, new_likelihood)\n",
    "        new_params = np.where(np.expand_dims(swap_matrice_1, -1), left_rolled_params, params)\n",
    "        new_params = np.where(np.expand_dims(swap_matrice_2, -1), right_rolled_params, new_params)\n",
    "\n",
    "        return new_params, new_prior, new_likelihood, accept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d543b16",
   "metadata": {},
   "source": [
    "<H3>Random Sampling</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSampling(OptimizationAlgorithm):\n",
    "\n",
    "    def __init__(self, loss_func):\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "\n",
    "    def run(self, repetitions, initial_parameters, parameter_lower_bounds, parameter_upper_bounds):\n",
    "        \n",
    "        initial_parameters = np.array(initial_parameters)\n",
    "        parameter_lower_bounds = np.array(parameter_lower_bounds)\n",
    "        parameter_upper_bounds = np.array(parameter_upper_bounds)\n",
    "\n",
    "        assert initial_parameters.shape == parameter_upper_bounds.shape and parameter_upper_bounds.shape == parameter_lower_bounds.shape\n",
    "\n",
    "        shape = initial_parameters.shape\n",
    "        best_solution = None\n",
    "        best_score = np.inf\n",
    "\n",
    "        bounds_diff = parameter_upper_bounds - parameter_lower_bounds\n",
    "\n",
    "        for _ in tqdm(range(repetitions)):\n",
    "\n",
    "            new_proposal = np.random.uniform(low=0, high=1, size=shape)\n",
    "            new_proposal = parameter_lower_bounds + (new_proposal * (bounds_diff))\n",
    "\n",
    "            score = self.loss_func(new_proposal)\n",
    "\n",
    "            if score <= best_score:\n",
    "                best_solution = new_proposal\n",
    "                best_score = score\n",
    "        \n",
    "        return best_solution, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db1c41",
   "metadata": {},
   "source": [
    "<H3>Scipy Optimizer</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "class ScipyOptimization(OptimizationAlgorithm):\n",
    "\n",
    "    def __init__(self, loss_func):\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "\n",
    "    def run(self, initial_parameters, method=\"powell\", bounds=None, tol=10 ** (-10), options = {}):\n",
    "\n",
    "        result = minimize(self.loss_func, x0 = initial_parameters, method=method, tol = tol, bounds=bounds, options=options)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6c23b",
   "metadata": {},
   "source": [
    "<H3>Evolutionary Algorithm</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5126965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvolutionaryOptimization(OptimizationAlgorithm):\n",
    "\n",
    "    def __init__(self, loss_func):\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "        self.known_implementations = [\"random\"]\n",
    "\n",
    "    def run(self, parameter_lower_bounds, parameter_upper_bounds, groups = 10, group_size = 100, generations = 100, steps_per_generation=50, replacement_amount = 50, variance = 1, replacement_mode = \"random\"):\n",
    "        \n",
    "        shape = (groups, group_size, len(parameter_lower_bounds))\n",
    "        #init generations\n",
    "        generation_lower_bounds = np.tile(np.array(parameter_lower_bounds).reshape((1,1,len(parameter_lower_bounds))), (groups, group_size, 1))\n",
    "        generation_upper_bounds = np.tile(np.array(parameter_upper_bounds).reshape((1,1,len(parameter_upper_bounds))), (groups, group_size, 1))\n",
    "\n",
    "        generation_bounds_diff = generation_upper_bounds - generation_lower_bounds\n",
    "        generation_params = generation_lower_bounds + (np.random.uniform(low=0, high=1,size=(groups, group_size, len(parameter_lower_bounds))) * generation_bounds_diff)\n",
    "\n",
    "        #init generation scores\n",
    "        generation_score = np.apply_along_axis(self.loss_func, axis=2, arr=generation_params)\n",
    "\n",
    "        for _ in tqdm(range(generations)):\n",
    "            \n",
    "            #params anpassen\n",
    "            for _ in range(steps_per_generation):\n",
    "            \n",
    "                #Erstelle werte von 0.9-1.1 die dann als skalierungswert für die parameter verwendet werden\n",
    "                #multiplcative_values = np.clip(np.random.normal(loc=1.0, scale=variance, size=shape), a_min=0.9, a_max=1.1)\n",
    "                #generation_proposal = generation_params * multiplcative_values\n",
    "\n",
    "                move = np.random.normal(loc=0, scale=variance, size=shape)\n",
    "                generation_proposal = generation_params + move\n",
    "                \n",
    "                #berechne improvment aus neuen param vorschlag. differenz zwischen neuen und alten score. Wenn diff > 0 dann neuer score niedriger\n",
    "                proposal_score = np.apply_along_axis(self.loss_func, axis=2, arr=generation_proposal)\n",
    "                improvments = generation_score-proposal_score\n",
    "                mask = (improvments > 0)\n",
    "\n",
    "                #Ersetze alle elemente, wo der neue vorschlag besser war\n",
    "                generation_params[mask] = generation_proposal[mask]\n",
    "                generation_score[mask] = proposal_score[mask]\n",
    "            \n",
    "            #finde die parameter mit dem höchsten score\n",
    "            generation_replace_indices = np.argpartition(a=generation_score, axis=1, kth=replacement_amount)\n",
    "\n",
    "            #gehe alle gruppen durch\n",
    "            for group in range(groups):\n",
    "                group_replace_indices = generation_replace_indices[group][replacement_amount+1:]\n",
    "                \n",
    "                possible_fill_in_indices = None\n",
    "\n",
    "                #case das mode \"random\" die indexe zum ersetzen auswählt\n",
    "                if replacement_mode == \"random\":\n",
    "                    possible_fill_in_indices = np.setdiff1d(ar1=np.arange(start=0, stop=group_size, step=1), ar2=group_replace_indices)\n",
    "                \n",
    "                else:\n",
    "                    raise NotImplementedError(f\"This replacement method is not implemented. Implemented are {self.known_implementations}!\")\n",
    "                \n",
    "                for replace_index in group_replace_indices:\n",
    "                    fill_in_index = np.random.choice(a=possible_fill_in_indices)\n",
    "                    generation_params[group][replace_index] = generation_params[group][fill_in_index]\n",
    "                    generation_score[group][replace_index] = generation_score[group][fill_in_index]\n",
    "        \n",
    "        minimal_index = np.argmin(a=generation_score)\n",
    "        shaped_minimal_index = np.unravel_index(minimal_index, generation_score.shape)\n",
    "\n",
    "        return generation_params[shaped_minimal_index[0]][shaped_minimal_index[1]], generation_score[shaped_minimal_index[0]][shaped_minimal_index[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee6e27b",
   "metadata": {},
   "source": [
    "<H1>Define Optimization Measurement</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36673561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_Algorithm(model: OptimizationAlgorithm, *args):\n",
    "    start = time.time()\n",
    "    result = model.run(*args)\n",
    "    end = time.time()\n",
    "    execution_time = end - start\n",
    "\n",
    "    return (execution_time, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf8743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_inputs ):\n",
    "        cur_data = data_inputs\n",
    "        inducer_concentration = cur_data[\"inducer concentrations\"]\n",
    "        output_RPU = cur_data[\"output\"]\n",
    "\n",
    "        data = inducer_concentration, list(map(lambda elem: np.median(elem), output_RPU))\n",
    "        X, Y_true = data\n",
    "        min_Y_true = np.nanmin(Y_true)\n",
    "        max_Y_true = np.nanmax(Y_true)\n",
    "        \n",
    "        return data, min_Y_true, max_Y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeccaec0",
   "metadata": {},
   "source": [
    "<H3>Random Sampling</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling_execute(data_inputs, algo_data):\n",
    "    np.seterr(all='ignore')\n",
    "    for construct_id in data_inputs:    \n",
    "            \n",
    "        data, min_Y_true, max_Y_true = prepare_data(data_inputs[construct_id])\n",
    "        \n",
    "        #Hillkurve initialparameter\n",
    "        init_params = (min_Y_true, max_Y_true, 2, 1)\n",
    "        params_lower_bounds = (min_Y_true * 10**(-1), max_Y_true * 10**(-1), 10**(-1), 10**(-1))\n",
    "        params_upper_bounds = (min_Y_true * 10**(1), max_Y_true * 10**(1), 10**(1), 10**(1))\n",
    "        \n",
    "        loss_func = get_loss_function(model=activatory_hill_equation, data=data)\n",
    "        \n",
    "        #Random Sampling\n",
    "        rs = RandomSampling(loss_func=loss_func)\n",
    "        rs_time, result = benchmark_Algorithm(rs, 2*10**6, init_params, params_lower_bounds, params_upper_bounds)\n",
    "        parameters, score = result\n",
    "\n",
    "        if not \"Randomsampling\" in algo_data:\n",
    "            algo_data[\"Randomsampling\"] = []\n",
    "\n",
    "        algo_data[\"Randomsampling\"].append((construct_id, rs_time, score))\n",
    "        \n",
    "        #print(f\"{rs_time=};{construct_id=};{parameters=};{score=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1426c032",
   "metadata": {},
   "source": [
    "<H3>Scipy Optimization</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f185ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scipy_optimization_execute(data_inputs, algo_data):\n",
    "    np.seterr(all='ignore')\n",
    "    for construct_id in data_inputs:    \n",
    "            \n",
    "        data, min_Y_true, max_Y_true = prepare_data(data_inputs[construct_id])\n",
    "        \n",
    "        #Hillkurve initialparameter\n",
    "        init_params = (min_Y_true, max_Y_true, 2, 1)\n",
    "\n",
    "        loss_func = get_loss_function(model=activatory_hill_equation, data=data)\n",
    "        \n",
    "        #Scipy Optimization\n",
    "        max_fev_per_param = 500\n",
    "\n",
    "        tol=10 ** (-10)\n",
    "        options={\"disp\": True,\n",
    "                \"ftol\": 10 ** (-7),\n",
    "                \"maxfev\": len(init_params) * max_fev_per_param,\n",
    "                \"disp\":False\n",
    "                }\n",
    "        \n",
    "        so = ScipyOptimization(loss_func=loss_func)\n",
    "        #Missing \"Newton-CG\", \"dogleg\",\"trust-constr\", \"trust-ncg\",\"trust-exact\", \"trust-krylov\"\n",
    "        methods = [\"Nelder-Mead\",\"Powell\",\"CG\",\"BFGS\", \"L-BFGS-B\",\n",
    "                \"TNC\", \"COBYLA\", \"COBYQA\", \"SLSQP\", \n",
    "                    ]\n",
    "\n",
    "        for method in methods:\n",
    "\n",
    "            so_time, result= benchmark_Algorithm(so, init_params, method, None, tol, options)\n",
    "\n",
    "            if not method in algo_data:\n",
    "                algo_data[method] = []\n",
    "\n",
    "            algo_data[method].append((construct_id, so_time, result['fun']))\n",
    "        \n",
    "        #print(f\"{construct_id=};parameters={result['x']}; score={result['fun']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed3c504",
   "metadata": {},
   "source": [
    "<H3>Parallel Tempering</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e0ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_tempering_execute(data_inputs, algo_data):\n",
    "    for construct_id in data_inputs:    \n",
    "            \n",
    "        data, min_Y_true, max_Y_true = prepare_data(data_inputs[construct_id])\n",
    "        \n",
    "        init_params = (min_Y_true, max_Y_true, 2, 1)\n",
    "\n",
    "        loss_func = get_loss_function(model=activatory_hill_equation, data=data)\n",
    "\n",
    "        def log_prior(params):\n",
    "            # Define more precise prior for y_max and y_min with a broad distribution (y_max, y_max * 2). Check how proxies for n and k can be inferred directly from data\n",
    "\n",
    "            a_min=np.log([max_Y_true, min_Y_true / 2, 10**(-20), 10**(-20)])\n",
    "            a_max=np.log([max_Y_true * 2, min_Y_true, 10**10, 10**10])\n",
    "            l_prior = - np.power(params - np.clip(params, a_min=a_min, a_max=a_max), 2) / 1\n",
    "            l_prior = np.sum(l_prior, axis=-1)    \n",
    "            \n",
    "            return l_prior        \n",
    "        \n",
    "        def log_likelihood(params):    \n",
    "            # Vectorization of loss function\n",
    "            if len(params.shape) == 1:\n",
    "                return -loss_func(params)\n",
    "            else:\n",
    "                log_likelihood_vals = np.empty(shape=params.shape[:2])\n",
    "                for i1 in range(params.shape[0]):\n",
    "                    for i2 in range(params.shape[1]):\n",
    "                        log_likelihood_vals[i1, i2] = -loss_func(params[i1, i2]) / 1\n",
    "                    \n",
    "                return log_likelihood_vals\n",
    "        \n",
    "        \n",
    "        \n",
    "        pt = ParallelTempering(log_likelihood=log_likelihood, log_prior=log_prior, n_dim=len(init_params), \n",
    "                            n_walkers=n_walkers,\n",
    "                            n_chains=n_chains)\n",
    "        pt_time, result = benchmark_Algorithm(pt, np.log(init_params), n_samples, 0.4, True)\n",
    "        parameters, priors, likelihoods, step_accepts, swap_accepts = result\n",
    "        best_index = np.unravel_index(np.argmax(priors + likelihoods), likelihoods.shape)\n",
    "        params = np.exp(parameters[*best_index])\n",
    "\n",
    "        best_loss = loss_func(np.log(params))\n",
    "        # ymax, ymin, n, k\n",
    "        params_dict = {\"y_max\": params[0],\n",
    "                    \"y_min\": params[1],\n",
    "                    \"n\": params[2],\n",
    "                    \"k\": params[3]}\n",
    "        data_inputs[construct_id][\"model information\"] = {\"model type\": \"ACTIVATORY HILL EQUATION\",\n",
    "                                                        \"calibration loss\": best_loss,\n",
    "                                                        \"parameters\" : params_dict}\n",
    "        \n",
    "        if not \"ParallelTempering\" in algo_data:\n",
    "            algo_data[\"ParallelTempering\"] = []\n",
    "\n",
    "        algo_data[\"ParallelTempering\"].append((construct_id, pt_time, best_loss))\n",
    "        #print(f\"{construct_id}; parameters={params_dict}; score={best_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b3e16c",
   "metadata": {},
   "source": [
    "<H3>Evolutionary Optimization </H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f13bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolutionary_optimization_execute(data_inputs, algo_data):\n",
    "    for construct_id in data_inputs:    \n",
    "            \n",
    "        data, min_Y_true, max_Y_true = prepare_data(data_inputs[construct_id])\n",
    "        \n",
    "        #Hillkurve initialparameter\n",
    "        init_params = (min_Y_true, max_Y_true, 2, 1)\n",
    "        params_lower_bounds = (min_Y_true * 10**(-1), max_Y_true * 10**(-1), 10**(-1), 10**(-1))\n",
    "        params_upper_bounds = (min_Y_true * 10**(1), max_Y_true * 10**(1), 10**(1), 10**(1))\n",
    "        \n",
    "\n",
    "        loss_func = get_loss_function(model=activatory_hill_equation, data=data)\n",
    "\n",
    "        #EvolutionaryOptimization\n",
    "        groups = 1\n",
    "        group_size = 100\n",
    "        generations = 100\n",
    "        steps_per_generation = 500\n",
    "        replacement_amount = 20\n",
    "        variance = 0.001\n",
    "        eo = EvolutionaryOptimization(loss_func=loss_func)\n",
    "        eo_time, result = benchmark_Algorithm(eo, params_lower_bounds, params_upper_bounds,\n",
    "                        groups, group_size, generations, steps_per_generation, replacement_amount, variance, \"random\")\n",
    "        params, score = result\n",
    "        \n",
    "        if not \"EvolutionaryOptimization\" in algo_data:\n",
    "            algo_data[\"EvolutionaryOptimization\"] = []\n",
    "\n",
    "        algo_data[\"EvolutionaryOptimization\"].append((construct_id, eo_time, score))\n",
    "\n",
    "        #print(f\"{construct_id=}; {params=}; {score=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec1610",
   "metadata": {},
   "source": [
    "<H1>Define Visualizer</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e83e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_plot_style():\n",
    "    plt.xlabel(\"Runtime in seconds\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(\n",
    "        bbox_to_anchor=(1.05, 1),\n",
    "        loc='upper left',\n",
    "        borderaxespad=0.          \n",
    "    )\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f5ac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(algo_data):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    markers = ['o', 's', 'v', '^', '<', '>', 'P', '*', 'X', 'D', 'h', '+']\n",
    "    colors = plt.cm.tab20.colors  # Up to 20 distinct colors\n",
    "\n",
    "    for i, (key, value) in enumerate(algo_data.items()):\n",
    "        run_label = f\"{key}\"\n",
    "\n",
    "        run_time = []\n",
    "        run_score = []\n",
    "        for run in value:\n",
    "            run_time.append(run[1])\n",
    "            run_score.append(run[2])\n",
    "\n",
    "        plt.scatter(run_time, run_score, label=run_label, color=colors[i % len(colors)], \n",
    "                    marker=markers[i % len(markers)])\n",
    "\n",
    "\n",
    "    default_plot_style()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ded490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results_per_run(algo_data):\n",
    "    data_per_run = {}\n",
    "    for i, (key, value) in enumerate(algo_data.items()):\n",
    "\n",
    "        run_time = []\n",
    "        run_score = []\n",
    "        for run in value:\n",
    "            \n",
    "            run_time.append(run[1])\n",
    "            run_score.append(run[2])\n",
    "\n",
    "            if not run[0] in data_per_run:\n",
    "                data_per_run[run[0]] = []\n",
    "            \n",
    "            data_per_run[run[0]].append((key, run[1], run[2]))\n",
    "\n",
    "    for i, (key, value) in enumerate(data_per_run.items()):\n",
    "\n",
    "        for point in value:\n",
    "            plt.scatter(point[1], point[2], label=point[0])\n",
    "        plt.title(key)\n",
    "        default_plot_style()\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2037356f",
   "metadata": {},
   "source": [
    "<H1>Execute Modelkalibration</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e24cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_input_data = {}\n",
    "\n",
    "\n",
    "scipy_optimization_execute(data_inputs, algo_input_data)\n",
    "random_sampling_execute(data_inputs, algo_input_data)\n",
    "parallel_tempering_execute(data_inputs, algo_input_data)\n",
    "evolutionary_optimization_execute(data_inputs, algo_input_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa37a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_output_data = {}\n",
    "\n",
    "scipy_optimization_execute(data_gates, algo_output_data)\n",
    "random_sampling_execute(data_gates, algo_output_data)\n",
    "parallel_tempering_execute(data_gates, algo_output_data)\n",
    "evolutionary_optimization_execute(data_gates, algo_output_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d477b5",
   "metadata": {},
   "source": [
    "<H1>Vizualise Results</H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea9dc48",
   "metadata": {},
   "source": [
    "<H3>Input Data</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5158dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(algo_input_data)\n",
    "visualize_results_per_run(algo_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f352ffd",
   "metadata": {},
   "source": [
    "<H3>Output Data</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06993996",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(algo_output_data)\n",
    "visualize_results_per_run(algo_output_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
